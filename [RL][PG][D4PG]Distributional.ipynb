{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aE6-lCM6oqv3",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import namedtuple\n",
        "\n",
        "version = \"1.0.0\"\n",
        "\n",
        "StepInfo = namedtuple(\"StepInfo\", (\"obs\", \"act_v\", \"act\", \"last_obs\", \"rew\", \"done\", \"etc\", \"n\"))\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, actor, noise = None, rend_wait = -1, rend_interval = -1                  , frame = None, max_step = None, device = None):\n",
        "        self.env = env\n",
        "        if max_step is not None:\n",
        "            self.env._max_episode_steps = max_step\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")            \n",
        "        self.actor = actor\n",
        "        self.noise = noise\n",
        "        self.device = device\n",
        "        \n",
        "        self.wait = rend_wait\n",
        "        self.interval = rend_interval\n",
        "        self.frame = frame\n",
        "        \n",
        "        self.step_set = False\n",
        "        self.n_step = 1\n",
        "        self.gamma = 0.99\n",
        "        \n",
        "    def set_n_step(self, n, gamma):\n",
        "        self.step_set = True\n",
        "        self.n_step = n\n",
        "        self.gamma= gamma\n",
        "        \n",
        "    def reset(self):\n",
        "        self.env.close()\n",
        "        self.env.reset()\n",
        "        \n",
        "    def render(self, epoch):\n",
        "        if self.wait >= 0 and epoch < self.wait:\n",
        "            return\n",
        "        if self.interval >= 0 and epoch % self.interval == 0:\n",
        "            rend = self.env.render(\"rgb_array\")\n",
        "            if self.frame is not None:\n",
        "                self.frame.append(self.env.render(\"rgb_array\"))\n",
        "        \n",
        "    def episode(self, epoch):\n",
        "        assert self.step_set\n",
        "        buffer = []\n",
        "        self.obs = self.env.reset()\n",
        "        self.render(epoch)\n",
        "\n",
        "        total_rew = 0\n",
        "        while True:\n",
        "            with torch.no_grad():\n",
        "                act_v = self.actor(torch.FloatTensor([self.obs])                                   .to(self.device)).cpu().squeeze(0).numpy()\n",
        "                if self.noise is not None:\n",
        "                    act_v += self.noise.get_noise()\n",
        "                if self.env.action_space.shape:\n",
        "                    act_v = act_v.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                act = self.actor.get_action(act_v)\n",
        "\n",
        "            next_obs, rew, done, etc = self.env.step(act)\n",
        "            rew/=10\n",
        "            total_rew += rew\n",
        "            if done and total_rew != -50:\n",
        "                rew = 10\n",
        "            self.render(epoch)\n",
        "\n",
        "            obs = self.obs\n",
        "            self.obs = next_obs\n",
        "\n",
        "            buffer.append(StepInfo(obs, act_v, act, next_obs, rew, done, etc, -1))\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "            if len(buffer) < self.n_step:\n",
        "                continue\n",
        "                \n",
        "            yield self.unroll_step(buffer)\n",
        "            buffer.pop(0)\n",
        "            \n",
        "        while len(buffer):\n",
        "            yield self.unroll_step(buffer)\n",
        "            buffer.pop(0)\n",
        "        print(epoch, \"%.5f\"%total_rew, end=' ')\n",
        "        return\n",
        "\n",
        "    def unroll_step(self, buffer):\n",
        "        assert len(buffer)\n",
        "        \n",
        "        rews = list(map(lambda b:b.rew, buffer))\n",
        "        rews.reverse()\n",
        "        rew_sum = 0\n",
        "\n",
        "        for r in rews:\n",
        "            rew_sum*=self.gamma\n",
        "            rew_sum+=r\n",
        "            \n",
        "        done = buffer[-1].done if len(buffer) == self.n_step else True\n",
        "        return StepInfo(buffer[0].obs, buffer[0].act_v, buffer[0].act, buffer[-1].last_obs, rew_sum, done, buffer[0].etc, len(buffer))\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class NoiseMaker():\n",
        "    def __init__(self, action_size, n_type = None, param = None, decay = True):\n",
        "        self.action_size = action_size\n",
        "        self.state = np.zeros(action_size, dtype=np.float32)\n",
        "        self.count = 0\n",
        "        self.decay = decay\n",
        "        if n_type is None:\n",
        "            n_type = \"normal\"\n",
        "        self.type = n_type\n",
        "        \n",
        "        if param is None:\n",
        "            self.param = {\n",
        "                \"start\": 0.9,\n",
        "                \"end\":0.02,\n",
        "                \"decay\": 2000\n",
        "            }\n",
        "            if n_type ==\"ou\":\n",
        "                self.param[\"ou_mu\"] = 0.0\n",
        "                self.param[\"ou_th\"] = 0.15\n",
        "                self.param[\"ou_sig\"] = 0.2\n",
        "        else:\n",
        "            self.param = param\n",
        "            \n",
        "    def get_noise(self):\n",
        "        eps = self.param[\"end\"] + (self.param[\"start\"] - self.param[\"end\"]) * math.exp(-1*self.count/ self.param[\"decay\"])\n",
        "        \n",
        "        noise = np.random.normal(size=self.action_size)\n",
        "        if self.type == \"ou\":\n",
        "            self.state += self.param[\"ou_th\"] * (self.param[\"ou_mu\"] - self.state) + self.param[\"ou_sig\"] * noise\n",
        "            noise = self.state\n",
        "        if not self.decay:\n",
        "            eps = 1\n",
        "        self.count += 1\n",
        "            \n",
        "        return noise * eps\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Replay:\n",
        "    def __init__(self, size, prio = False, alph = 0.6, beta = 0.4):\n",
        "        self.memory = collections.deque(maxlen = size)\n",
        "        self.size = size\n",
        "        self.priorities = collections.deque(maxlen = size)\n",
        "        self.prio = prio\n",
        "        self.alph = alph if prio else 0\n",
        "        self.beta = beta if prio else 0\n",
        "        self.count = 0\n",
        "        \n",
        "    def push(self, data):\n",
        "        self.memory.append(data)\n",
        "        self.count += 1\n",
        "        if self.prio:\n",
        "            max_prio = np.array(self.priorities).max() if len(self.priorities) else 1.\n",
        "            self.priorities.append(max_prio)\n",
        "        \n",
        "    def prepare(self, env):\n",
        "        pass\n",
        "        \n",
        "    def sample(self, size):\n",
        "        if self.prio:\n",
        "            probs = np.array(self.priorities, dtype=np.float32)\n",
        "            min_prio = np.array(self.priorities).min()\n",
        "            if min_prio < 1:\n",
        "                probs /= min_prio\n",
        "            probs = probs ** self.alph\n",
        "            probs /= probs.sum()\n",
        "        else:\n",
        "            probs = np.ones(len(self),) / len(self)\n",
        "        \n",
        "        indices = np.random.choice(len(self), size, p=probs)\n",
        "        sample = [self.memory[idx] for idx in indices]\n",
        "        \n",
        "        beta = 1. + (self.beta - 1.) * math.exp(-1 * self.count / self.size)\n",
        "        weights = (len(self) * probs[indices]) ** (-beta)\n",
        "        weights /= weights.sum()\n",
        "        \n",
        "        return sample, indices, weights\n",
        "        \n",
        "    def update_priorities(self, indices, prios):\n",
        "        if not self.prio:\n",
        "            return\n",
        "        prios += 1e-8\n",
        "        for idx, prio in zip(indices,prios):\n",
        "            self.priorities[idx] = prio\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vYxEy8Q6oqwJ",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import math\n",
        "\n",
        "class NoiseLinear(nn.Linear):\n",
        "    def __init__(self, in_, out_, val = 0.017, bias = True):\n",
        "        super(NoiseLinear, self).__init__(in_,out_,bias)\n",
        "        self.sigma_weight = nn.Parameter(torch.full((out_, in_), val))\n",
        "        self.register_buffer(\"eps_weight\", torch.zeros(out_, in_))\n",
        "        if bias:\n",
        "            self.sigma_bias = nn.Parameter(torch.full((out_,), val))\n",
        "            self.register_buffer(\"eps_bias\", torch.zeros(out_))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = math.sqrt(1 / self.in_features)\n",
        "        self.weight.data.uniform_(-std, std)\n",
        "        self.bias.data.uniform_(-std, std)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.eps_weight.normal_()\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            self.eps_bias.normal_()\n",
        "            bias = bias + self.sigma_bias * self.eps_bias.data\n",
        "        return F.linear(x, self.weight + self.sigma_weight * self.eps_weight, bias)\n",
        "\n",
        "class targetNet(nn.Module):\n",
        "    def __init__(self, off_net):\n",
        "        super(targetNet, self).__init__()\n",
        "        self.net = copy.deepcopy(off_net)\n",
        "        self.off_net = off_net\n",
        "        \n",
        "    def alpha_update(self, alpha = 0.05):\n",
        "        for off, tgt in zip(self.off_net.parameters(), self.net.parameters()):\n",
        "            tgt.data.copy_(off.data*alpha + tgt.data*(1-alpha))\n",
        "    \n",
        "    def copy_off_net(self):\n",
        "        self.net.load_state_dict(self.off_net.state_dict())\n",
        "    \n",
        "    def forward(self, *x):\n",
        "        return self.net(*x)\n",
        "        \n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, in_, act_n, action_provider, hidden=512):\n",
        "        super(Actor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, int(hidden/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(hidden/2), act_n),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.action_provider = action_provider\n",
        "        \n",
        "    def get_action(self, act_v):\n",
        "        return self.action_provider(act_v)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DistCritic(nn.Module):\n",
        "    def __init__(self, in_, act_v, atom=51, hidden=512):\n",
        "        super(DistCritic, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_, hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.net_out = nn.Sequential(\n",
        "            nn.Linear(hidden + act_n, int(hidden/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(hidden/2), atom)\n",
        "        )\n",
        "    \n",
        "    def forward(self, obs, act):\n",
        "        return self.net_out(torch.cat([self.net(obs), act], dim=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DxDEeOCfoqwd",
        "colab": {}
      },
      "source": [
        "ACT_LR = 0.001\n",
        "CRT_LR = 0.001\n",
        "\n",
        "GAMMA = 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LJrRlHDroqwm",
        "colab": {}
      },
      "source": [
        "V_MAX = 10\n",
        "V_MIN = -10\n",
        "ATOMS = 51\n",
        "V_DIST = (V_MAX-V_MIN)/(ATOMS-1)\n",
        "\n",
        "def transform_dist(dist, rew, gamma, unroll_n, done):\n",
        "    support_start = V_MIN\n",
        "    \n",
        "    dist = dist.cpu().detach().numpy()\n",
        "    ret = np.zeros_like(dist)\n",
        "    rew = rew.cpu().numpy()\n",
        "    unroll_n = unroll_n.cpu().numpy()\n",
        "    done = done.cpu().numpy()\n",
        "    for atom in range(ATOMS):\n",
        "        support = support_start + atom * V_DIST\n",
        "        next_support = rew + (gamma**unroll_n) * support\n",
        "        next_support[next_support > V_MAX] = V_MAX\n",
        "        next_support[next_support < V_MIN] = V_MIN\n",
        "\n",
        "        indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
        "        l = np.floor(indices).astype(np.int64)\n",
        "        r = np.ceil(indices).astype(np.int64)\n",
        "\n",
        "        eq = l==r\n",
        "        ret[eq, l[eq]] += dist[eq, atom]\n",
        "        neq = l!=r\n",
        "        ret[neq, l[neq]] += dist[neq, atom] * (r - indices)[neq]\n",
        "        ret[neq, r[neq]] += dist[neq, atom] * (indices - l)[neq]\n",
        "\n",
        "        if done.any():\n",
        "            ret[done] = 0.0\n",
        "            next_support = rew[done]\n",
        "            next_support[next_support > V_MAX] = V_MAX\n",
        "            next_support[next_support < V_MIN] = V_MIN\n",
        "\n",
        "            indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
        "            l = np.floor(indices).astype(np.int64)\n",
        "            r = np.ceil(indices).astype(np.int64)\n",
        "\n",
        "            eq = l==r\n",
        "            eq_done = done.copy()\n",
        "            eq_done[done] = eq\n",
        "            if eq_done.any():\n",
        "                ret[eq_done, l[eq]] = 1.0\n",
        "\n",
        "            neq = l!=r\n",
        "            neq_done = done.copy()\n",
        "            neq_done[done] = neq\n",
        "            if neq_done.any():\n",
        "                ret[neq_done, l[neq]] = (r - indices)[neq]\n",
        "                ret[neq_done, r[neq]] = (indices - l)[neq]\n",
        "            \n",
        "    return torch.FloatTensor(ret).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WEA4NK63oqxI",
        "outputId": "5f4f1d67-ed5f-4e19-c3c3-7fb1a918420e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "env = gym.make(\"Acrobot-v1\")\n",
        "act_n = env.action_space.n\n",
        "obs_n = env.observation_space.shape[0]\n",
        "\n",
        "actor = Actor(obs_n, act_n, lambda x:x.argmax()).cuda()\n",
        "actor_tgt = targetNet(actor)\n",
        "actor_optim = optim.Adam(actor.parameters(), ACT_LR)\n",
        "\n",
        "critic = DistCritic(obs_n, act_n, ATOMS).cuda()\n",
        "critic_tgt = targetNet(critic)\n",
        "critic_optim = optim.Adam(critic.parameters(), CRT_LR)\n",
        "\n",
        "ST_SIZE = 50000\n",
        "ST_INIT = 10000\n",
        "BATCH = 512\n",
        "storage = Replay(ST_SIZE, True)\n",
        "noise = NoiseMaker(act_n, \"ou\", decay=False)\n",
        "noise.param[\"decay\"] = ST_SIZE\n",
        "\n",
        "agent = Agent(env, actor, noise)\n",
        "agent.set_n_step(3, GAMMA)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vvt1_9i0oqxQ",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d370296-09fd-4bd6-8171-561badbc5a99"
      },
      "source": [
        "import time\n",
        "EPOCH = 2000\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    for i, step in enumerate(agent.episode(epoch)):\n",
        "        storage.push(step)\n",
        "        if len(storage) < ST_INIT:\n",
        "            continue\n",
        "            \n",
        "        sample, indices, weights = storage.sample(BATCH)\n",
        "        weights_ = torch.FloatTensor(weights).cuda()\n",
        "        obs, act_v, act, next_obs, rew, done, etc, unroll_n = list(zip(*sample))\n",
        "        \n",
        "        obs_ = torch.FloatTensor(obs).cuda()\n",
        "        act_v_ = torch.FloatTensor(act_v).cuda()\n",
        "        act_ = torch.LongTensor(act).unsqueeze(1).cuda()\n",
        "        next_obs_ = torch.FloatTensor(next_obs).cuda()\n",
        "        rew_ = torch.FloatTensor(rew).unsqueeze(1).cuda()\n",
        "        done_ = torch.BoolTensor(done).cuda()\n",
        "        unroll_n_ = torch.FloatTensor(unroll_n).unsqueeze(1).cuda()\n",
        "\n",
        "        #Critic update\n",
        "        critic_optim.zero_grad()\n",
        "        q_pred = critic(obs_, act_v_)\n",
        "        \n",
        "        q_next_prob = critic_tgt(next_obs_, actor_tgt(next_obs_))\n",
        "        q_next = F.softmax(q_next_prob, dim=1)\n",
        "        q_target = transform_dist(q_next, rew_, GAMMA, unroll_n_, done_)\n",
        "\n",
        "        q_entropy = -F.log_softmax(q_pred, dim=1) * q_target\n",
        "        q_entropy_sum = q_entropy.sum(dim=1)\n",
        "        q_loss = (weights_ * q_entropy_sum).sum()\n",
        "        q_loss.backward()\n",
        "        critic_optim.step()\n",
        "\n",
        "        storage.update_priorities(indices, q_entropy_sum.cpu().detach().numpy())\n",
        "\n",
        "        #Actor update\n",
        "        actor_optim.zero_grad()\n",
        "        q_dist = critic(obs_,actor(obs_))\n",
        "        q_v = -F.softmax(q_dist,dim=1) * torch.arange(V_MIN, V_MAX + V_DIST, V_DIST).cuda()\n",
        "        q_v = q_v.mean(dim=1)\n",
        "\n",
        "        actor_loss = q_v.mean()\n",
        "        actor_loss.backward()\n",
        "        actor_optim.step()\n",
        "\n",
        "        #target update\n",
        "        critic_tgt.alpha_update()\n",
        "        actor_tgt.alpha_update()\n",
        "    print()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 -40.70000 \n",
            "1 -50.00000 \n",
            "2 -50.00000 \n",
            "3 -24.00000 \n",
            "4 -25.70000 \n",
            "5 -49.20000 \n",
            "6 -50.00000 \n",
            "7 -29.40000 \n",
            "8 -33.30000 \n",
            "9 -50.00000 \n",
            "10 -48.10000 \n",
            "11 -34.40000 \n",
            "12 -34.00000 \n",
            "13 -42.80000 \n",
            "14 -50.00000 \n",
            "15 -50.00000 \n",
            "16 -41.70000 \n",
            "17 -43.90000 \n",
            "18 -43.40000 \n",
            "19 -31.40000 \n",
            "20 -22.30000 \n",
            "21 -13.30000 \n",
            "22 -28.90000 \n",
            "23 -35.50000 \n",
            "24 -23.60000 \n",
            "25 -37.00000 \n",
            "26 -50.00000 \n",
            "27 -50.00000 \n",
            "28 -50.00000 \n",
            "29 -22.20000 \n",
            "30 -50.00000 \n",
            "31 -43.80000 \n",
            "32 -50.00000 \n",
            "33 -12.70000 \n",
            "34 -21.00000 \n",
            "35 -13.60000 \n",
            "36 -11.10000 \n",
            "37 -10.60000 \n",
            "38 -14.00000 \n",
            "39 -15.60000 \n",
            "40 -15.90000 \n",
            "41 -11.10000 \n",
            "42 -11.70000 \n",
            "43 -9.10000 \n",
            "44 -12.50000 \n",
            "45 -9.90000 \n",
            "46 -8.90000 \n",
            "47 -25.00000 \n",
            "48 -13.00000 \n",
            "49 -27.50000 \n",
            "50 -18.50000 \n",
            "51 -12.40000 \n",
            "52 -13.90000 \n",
            "53 -9.80000 \n",
            "54 -13.50000 \n",
            "55 -14.50000 \n",
            "56 -10.60000 \n",
            "57 -15.00000 \n",
            "58 -14.80000 \n",
            "59 -11.90000 \n",
            "60 -13.70000 \n",
            "61 -10.30000 \n",
            "62 -15.90000 \n",
            "63 -8.50000 \n",
            "64 -10.00000 \n",
            "65 -10.10000 \n",
            "66 -13.70000 \n",
            "67 -10.90000 \n",
            "68 -10.40000 \n",
            "69 -16.80000 \n",
            "70 -12.80000 \n",
            "71 -9.30000 \n",
            "72 -9.40000 \n",
            "73 -9.30000 \n",
            "74 -8.20000 \n",
            "75 -7.80000 \n",
            "76 -7.40000 \n",
            "77 -7.50000 \n",
            "78 -9.70000 \n",
            "79 -9.40000 \n",
            "80 -9.60000 \n",
            "81 -8.30000 \n",
            "82 -7.80000 \n",
            "83 -8.60000 \n",
            "84 -10.90000 \n",
            "85 -9.50000 \n",
            "86 -9.60000 \n",
            "87 -7.80000 \n",
            "88 -7.80000 \n",
            "89 -7.90000 \n",
            "90 -9.00000 \n",
            "91 -7.90000 \n",
            "92 -8.20000 \n",
            "93 -8.20000 \n",
            "94 -9.30000 \n",
            "95 -9.50000 \n",
            "96 -6.60000 \n",
            "97 -10.00000 \n",
            "98 -6.70000 \n",
            "99 -9.70000 \n",
            "100 -11.10000 \n",
            "101 -7.30000 \n",
            "102 -9.60000 \n",
            "103 -7.80000 \n",
            "104 -9.10000 \n",
            "105 -10.00000 \n",
            "106 -8.50000 \n",
            "107 -7.90000 \n",
            "108 -8.10000 \n",
            "109 -8.20000 \n",
            "110 -9.40000 \n",
            "111 -7.50000 \n",
            "112 -7.60000 \n",
            "113 -8.20000 \n",
            "114 -7.50000 \n",
            "115 -16.40000 \n",
            "116 -9.20000 \n",
            "117 -8.80000 \n",
            "118 -9.90000 \n",
            "119 -8.30000 \n",
            "120 -8.20000 \n",
            "121 -6.90000 \n",
            "122 -7.70000 \n",
            "123 -8.10000 \n",
            "124 -8.70000 \n",
            "125 -7.90000 \n",
            "126 -10.30000 \n",
            "127 -19.70000 \n",
            "128 -8.40000 \n",
            "129 -10.40000 \n",
            "130 -8.10000 \n",
            "131 -9.70000 \n",
            "132 -9.90000 \n",
            "133 -10.20000 \n",
            "134 -9.20000 \n",
            "135 -7.80000 \n",
            "136 -9.30000 \n",
            "137 -7.60000 \n",
            "138 -8.70000 \n",
            "139 -9.00000 \n",
            "140 -12.00000 \n",
            "141 -11.50000 \n",
            "142 -8.70000 \n",
            "143 -10.20000 \n",
            "144 -10.90000 \n",
            "145 -9.60000 \n",
            "146 -10.70000 \n",
            "147 -6.70000 \n",
            "148 -8.90000 \n",
            "149 -9.60000 \n",
            "150 -6.60000 \n",
            "151 -9.90000 \n",
            "152 -10.60000 \n",
            "153 -8.10000 \n",
            "154 -8.20000 \n",
            "155 -8.80000 \n",
            "156 -8.40000 \n",
            "157 -8.80000 \n",
            "158 -8.90000 \n",
            "159 -10.70000 \n",
            "160 -8.70000 \n",
            "161 -8.70000 \n",
            "162 -9.70000 \n",
            "163 -7.80000 \n",
            "164 -10.10000 \n",
            "165 -13.30000 \n",
            "166 -8.70000 \n",
            "167 -6.90000 \n",
            "168 -8.30000 \n",
            "169 -8.50000 \n",
            "170 -8.00000 \n",
            "171 -6.90000 \n",
            "172 -8.90000 \n",
            "173 -7.50000 \n",
            "174 -9.00000 \n",
            "175 -9.00000 \n",
            "176 -6.80000 \n",
            "177 -9.20000 \n",
            "178 -14.80000 \n",
            "179 -9.10000 \n",
            "180 -8.30000 \n",
            "181 -7.20000 \n",
            "182 -10.40000 \n",
            "183 -11.10000 \n",
            "184 -10.00000 \n",
            "185 -8.30000 \n",
            "186 -9.80000 \n",
            "187 -6.90000 \n",
            "188 -10.20000 \n",
            "189 -6.20000 \n",
            "190 -11.00000 \n",
            "191 -6.90000 \n",
            "192 -8.70000 \n",
            "193 -9.70000 \n",
            "194 -11.70000 \n",
            "195 -9.20000 \n",
            "196 -6.90000 \n",
            "197 -10.10000 \n",
            "198 -6.80000 \n",
            "199 -7.80000 \n",
            "200 -6.90000 \n",
            "201 -6.90000 \n",
            "202 -12.40000 \n",
            "203 -7.60000 \n",
            "204 -6.70000 \n",
            "205 -7.70000 \n",
            "206 -10.70000 \n",
            "207 -6.20000 \n",
            "208 -8.30000 \n",
            "209 -10.10000 \n",
            "210 -8.70000 \n",
            "211 -7.40000 \n",
            "212 -11.00000 \n",
            "213 -7.70000 \n",
            "214 -11.20000 \n",
            "215 -6.60000 \n",
            "216 -10.50000 \n",
            "217 -14.10000 \n",
            "218 -9.70000 \n",
            "219 -16.20000 \n",
            "220 -8.30000 \n",
            "221 -6.00000 \n",
            "222 -9.10000 \n",
            "223 -7.30000 \n",
            "224 -7.70000 \n",
            "225 -6.80000 \n",
            "226 -10.70000 \n",
            "227 -9.20000 \n",
            "228 -8.40000 \n",
            "229 -8.10000 \n",
            "230 -6.20000 \n",
            "231 -14.70000 \n",
            "232 -6.10000 \n",
            "233 -9.10000 \n",
            "234 -10.10000 \n",
            "235 -10.20000 \n",
            "236 -6.90000 \n",
            "237 -7.90000 \n",
            "238 -10.20000 \n",
            "239 -7.80000 \n",
            "240 -7.80000 \n",
            "241 -9.80000 \n",
            "242 -6.90000 \n",
            "243 -9.90000 \n",
            "244 -6.20000 \n",
            "245 -9.50000 \n",
            "246 -8.50000 \n",
            "247 -6.00000 \n",
            "248 -8.40000 \n",
            "249 -9.60000 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-b6d02d4f65af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mST_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-08236501f537>\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mmax_prio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_prio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}