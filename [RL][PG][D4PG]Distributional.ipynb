{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aE6-lCM6oqv3"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYxEy8Q6oqwJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import math\n",
    "\n",
    "class NoiseLinear(nn.Linear):\n",
    "    def __init__(self, in_, out_, val = 0.017, bias = True):\n",
    "        super(NoiseLinear, self).__init__(in_,out_,bias)\n",
    "        self.sigma_weight = nn.Parameter(torch.full((out_, in_), val))\n",
    "        self.register_buffer(\"eps_weight\", torch.zeros(out_, in_))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.full((out_,), val))\n",
    "            self.register_buffer(\"eps_bias\", torch.zeros(out_))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(1 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.eps_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.eps_bias.normal_()\n",
    "            bias = bias + self.sigma_bias * self.eps_bias.data\n",
    "        return F.linear(x, self.weight + self.sigma_weight * self.eps_weight, bias)\n",
    "\n",
    "class targetNet(nn.Module):\n",
    "    def __init__(self, off_net):\n",
    "        super(targetNet, self).__init__()\n",
    "        self.net = copy.deepcopy(off_net)\n",
    "        self.off_net = off_net\n",
    "        \n",
    "    def alpha_update(self, alpha = 0.05):\n",
    "        for off, tgt in zip(self.off_net.parameters(), self.net.parameters()):\n",
    "            tgt.data.copy_(off.data*alpha + tgt.data*(1-alpha))\n",
    "    \n",
    "    def copy_off_net(self):\n",
    "        self.net.load_state_dict(self.off_net.state_dict())\n",
    "    \n",
    "    def forward(self, *x):\n",
    "        return self.net(*x)\n",
    "        \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_, act_n, hidden=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, act_n),\n",
    "        )\n",
    "        \n",
    "    def get_action(self, act_v):\n",
    "        return act_v\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DistCritic(nn.Module):\n",
    "    def __init__(self, in_, act_v, atom=51, hidden=256):\n",
    "        super(DistCritic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_, hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.net_out = nn.Sequential(\n",
    "            nn.Linear(hidden + act_n, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, atom)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        return self.net_out(torch.cat([self.net(obs), act], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxDEeOCfoqwd"
   },
   "outputs": [],
   "source": [
    "ACT_LR = 0.0001\n",
    "CRT_LR = 0.0005\n",
    "\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJrRlHDroqwm"
   },
   "outputs": [],
   "source": [
    "V_MAX = 20\n",
    "V_MIN = -20\n",
    "ATOMS = 51\n",
    "V_DIST = (V_MAX-V_MIN)/(ATOMS-1)\n",
    "\n",
    "def transform_dist(dist, rew, gamma, unroll_n, done):\n",
    "    support_start = V_MIN\n",
    "    \n",
    "    dist = dist.cpu().detach().numpy()\n",
    "    ret = np.zeros_like(dist)\n",
    "    rew = rew.cpu().numpy()\n",
    "    unroll_n = unroll_n.cpu().numpy()\n",
    "    done = done.cpu().numpy()\n",
    "    for atom in range(ATOMS):\n",
    "        support = support_start + atom * V_DIST\n",
    "        next_support = rew + (gamma**unroll_n) * support\n",
    "        next_support[next_support > V_MAX] = V_MAX\n",
    "        next_support[next_support < V_MIN] = V_MIN\n",
    "\n",
    "        indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
    "        l = np.floor(indices).astype(np.int64)\n",
    "        r = np.ceil(indices).astype(np.int64)\n",
    "\n",
    "        eq = l==r\n",
    "        ret[eq, l[eq]] += dist[eq, atom]\n",
    "        neq = l!=r\n",
    "        ret[neq, l[neq]] += dist[neq, atom] * (r - indices)[neq]\n",
    "        ret[neq, r[neq]] += dist[neq, atom] * (indices - l)[neq]\n",
    "\n",
    "        if done.any():\n",
    "            ret[done] = 0.0\n",
    "            next_support = rew[done]\n",
    "            next_support[next_support > V_MAX] = V_MAX\n",
    "            next_support[next_support < V_MIN] = V_MIN\n",
    "\n",
    "            indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
    "            l = np.floor(indices).astype(np.int64)\n",
    "            r = np.ceil(indices).astype(np.int64)\n",
    "\n",
    "            eq = l==r\n",
    "            eq_done = done.copy()\n",
    "            eq_done[done] = eq\n",
    "            if eq_done.any():\n",
    "                ret[eq_done, l[eq]] = 1.0\n",
    "\n",
    "            neq = l!=r\n",
    "            neq_done = done.copy()\n",
    "            neq_done[done] = neq\n",
    "            if neq_done.any():\n",
    "                ret[neq_done, l[neq]] = (r - indices)[neq]\n",
    "                ret[neq_done, r[neq]] = (indices - l)[neq]\n",
    "            \n",
    "    return torch.FloatTensor(ret).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "WEA4NK63oqxI",
    "outputId": "e49cae11-8ef2-4008-f14c-9642ad9683c4"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "act_n = env.action_space.shape[0]\n",
    "obs_n = env.observation_space.shape[0]\n",
    "\n",
    "actor = Actor(obs_n, act_n).cuda()\n",
    "actor_tgt = targetNet(actor)\n",
    "actor_optim = optim.Adam(actor.parameters(), ACT_LR)\n",
    "\n",
    "critic = DistCritic(obs_n, act_n, ATOMS).cuda()\n",
    "critic_tgt = targetNet(critic)\n",
    "critic_optim = optim.Adam(critic.parameters(), CRT_LR)\n",
    "\n",
    "ST_SIZE = 50000\n",
    "ST_INIT = 10000\n",
    "BATCH = 512\n",
    "storage = Replay(ST_SIZE, True)\n",
    "noise = NoiseMaker(act_n, \"ou\", decay=True)\n",
    "noise.param[\"decay\"] = ST_SIZE\n",
    "\n",
    "agent = Agent(env, actor, noise, 200, 1)\n",
    "agent.set_n_step(2, GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vvt1_9i0oqxQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -855.42269 \n",
      "1 -1748.63874 \n",
      "2 -850.76813 \n",
      "3 -865.73192 \n",
      "4 -1069.79105 \n",
      "5 -1667.22458 \n",
      "6 -1512.76431 \n",
      "7 -1445.10003 \n",
      "8 -1169.07088 \n",
      "9 -1575.89961 \n",
      "10 -986.86838 \n",
      "11 -1491.19924 \n",
      "12 -961.27757 \n",
      "13 -1293.81928 \n",
      "14 -968.86290 \n",
      "15 -1278.34625 \n",
      "16 -783.98711 \n",
      "17 -1664.46951 \n",
      "18 -1447.51600 \n",
      "19 -753.97188 \n",
      "20 -1731.16238 \n",
      "21 -1089.48874 \n",
      "22 -1470.78677 \n",
      "23 -918.15091 \n",
      "24 -1792.64299 \n",
      "25 -1021.93554 \n",
      "26 -1579.49300 \n",
      "27 -1172.04712 \n",
      "28 -1395.47010 \n",
      "29 -1400.34475 \n",
      "30 -1250.95015 \n",
      "31 -1169.67890 \n",
      "32 -1516.22624 \n",
      "33 -1297.65744 \n",
      "34 -1549.69517 \n",
      "35 -1724.44230 \n",
      "36 -1589.37821 \n",
      "37 -1058.61556 \n",
      "38 -926.60655 \n",
      "39 -963.18824 \n",
      "40 -1700.29890 \n",
      "41 -991.90680 \n",
      "42 -1148.20777 \n",
      "43 -1138.64827 \n",
      "44 -1326.73774 \n",
      "45 -1826.94966 \n",
      "46 -1677.00062 \n",
      "47 -1692.87860 \n",
      "48 -1107.36377 \n",
      "49 -1077.69971 \n",
      "50 -1602.51753 \n",
      "51 -1159.66959 \n",
      "52 -1121.87536 \n",
      "53 -1581.67320 \n",
      "54 -1471.61078 \n",
      "55 -938.49375 \n",
      "56 -1434.01179 \n",
      "57 -1496.79673 \n",
      "58 -1420.23193 \n",
      "59 -1364.15904 \n",
      "60 -1473.60496 \n",
      "61 -921.22945 \n",
      "62 -1066.13030 \n",
      "63 -1492.45345 \n",
      "64 -1639.11758 \n",
      "65 -1657.33253 \n",
      "66 -1509.38425 \n",
      "67 -1495.14125 \n",
      "68 -1633.08221 \n",
      "69 -1578.91924 \n",
      "70 -1494.23880 \n",
      "71 -1443.97923 \n",
      "72 -1288.91014 \n",
      "73 -1652.04425 \n",
      "74 -1613.87994 \n",
      "75 -1071.32320 \n",
      "76 -1648.01437 \n",
      "77 -1580.13193 \n",
      "78 -1498.37423 \n",
      "79 -1505.07889 \n",
      "80 -933.03879 \n",
      "81 -1003.03726 \n",
      "82 -1494.20575 \n",
      "83 -1496.42623 \n",
      "84 -1228.95260 \n",
      "85 -1644.93111 \n",
      "86 -1255.52319 \n",
      "87 -1495.77288 \n",
      "88 -1216.44995 \n",
      "89 -1377.16734 \n",
      "90 -1510.62465 \n",
      "91 -1459.61001 \n",
      "92 -1349.45379 \n",
      "93 -1519.16208 \n",
      "94 -1233.73333 \n",
      "95 -1505.27805 \n",
      "96 -1507.76504 \n",
      "97 -1522.16603 \n",
      "98 -1376.76702 \n",
      "99 -1474.48848 \n",
      "100 -1516.91708 \n",
      "101 -1494.36449 \n",
      "102 -1493.76924 \n",
      "103 -1483.77645 \n",
      "104 -1510.14382 \n",
      "105 -1523.17984 \n",
      "106 -1490.35374 \n",
      "107 -1512.98755 \n",
      "108 -1480.33586 \n",
      "109 -1529.35287 \n",
      "110 -1515.37649 \n",
      "111 -1500.57964 \n",
      "112 -1415.67411 \n",
      "113 -1539.58159 \n",
      "114 -1519.15702 \n",
      "115 -1506.93982 \n",
      "116 -1509.92893 \n",
      "117 -1580.27345 \n",
      "118 -1485.97088 \n",
      "119 -1493.63039 \n",
      "120 -1595.83765 \n",
      "121 -1753.08057 \n",
      "122 -1839.61609 \n",
      "123 -1719.22311 \n",
      "124 -1857.36169 \n",
      "125 -1810.94825 \n",
      "126 -1750.10826 \n",
      "127 -1732.57754 \n",
      "128 -1666.51286 \n",
      "129 -1.68137 \n",
      "130 -1765.75029 \n",
      "131 -1697.59136 \n",
      "132 -1672.86444 \n",
      "133 -0.77524 \n",
      "134 -1629.64483 \n",
      "135 -1682.26337 \n",
      "136 -1586.40568 \n",
      "137 -1750.27242 \n",
      "138 -1685.54511 \n",
      "139 -1752.49665 \n",
      "140 -2.77702 \n",
      "141 -1736.46618 \n",
      "142 -1650.67944 \n",
      "143 -1647.57062 \n",
      "144 -1.87368 \n",
      "145 -1550.39407 \n",
      "146 -1663.55938 \n",
      "147 -1733.37325 \n",
      "148 -1306.56129 \n",
      "149 -1520.15546 \n",
      "150 -1292.71947 \n",
      "151 -1509.87601 \n",
      "152 -0.50908 \n",
      "153 -1308.68168 \n",
      "154 -1277.72152 \n",
      "155 -1301.52760 \n",
      "156 -126.91214 \n",
      "157 -120.84719 \n",
      "158 -2.76242 \n",
      "159 -1393.13894 \n",
      "160 -1060.35314 \n",
      "161 -237.24493 \n",
      "162 -1373.97552 \n",
      "163 -126.97935 \n",
      "164 -240.84816 \n",
      "165 -131.54901 \n",
      "166 -1194.73097 \n",
      "167 -126.47430 \n",
      "168 -952.68519 \n",
      "169 -129.78444 \n",
      "170 -0.49730 \n",
      "171 -1084.90278 \n",
      "172 -1276.86528 \n",
      "173 -1263.56454 \n",
      "174 -1224.27415 \n",
      "175 -1491.78035 \n",
      "176 -1301.70455 \n",
      "177 -1301.10743 \n",
      "178 -1336.34026 \n",
      "179 -1201.46882 \n",
      "180 -1495.12668 \n",
      "181 -3.89632 \n",
      "182 -1254.45776 \n",
      "183 -1372.64415 \n",
      "184 -1223.75410 \n",
      "185 -1253.91883 \n",
      "186 -1259.50371 \n",
      "187 -126.98495 \n",
      "188 -1024.49538 \n",
      "189 -1.29121 \n",
      "190 -1505.24360 \n",
      "191 -1231.64290 \n",
      "192 -1300.59235 \n",
      "193 -1270.73022 \n",
      "194 -117.58062 \n",
      "195 -1512.15998 \n",
      "196 -1366.21686 \n",
      "197 -120.74162 \n",
      "198 -1318.32365 \n",
      "199 -1155.83024 \n",
      "200 -1167.65302 \n",
      "201 -120.79892 \n",
      "202 -1377.42645 \n",
      "203 -1095.05230 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 2000\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i, step in enumerate(agent.episode(epoch)):\n",
    "        storage.push(step)\n",
    "        if len(storage) < ST_INIT:\n",
    "            continue\n",
    "            \n",
    "        sample, indices, weights = storage.sample(BATCH)\n",
    "        weights_ = torch.FloatTensor(weights).cuda()\n",
    "        obs, act_v, act, next_obs, rew, done, etc, unroll_n = list(zip(*sample))\n",
    "        \n",
    "        obs_ = torch.FloatTensor(obs).cuda()\n",
    "        act_v_ = torch.FloatTensor(act_v).cuda()\n",
    "        act_ = torch.LongTensor(act).unsqueeze(1).cuda()\n",
    "        next_obs_ = torch.FloatTensor(next_obs).cuda()\n",
    "        rew_ = torch.FloatTensor(rew).unsqueeze(1).cuda()\n",
    "        done_ = torch.BoolTensor(done).cuda()\n",
    "        unroll_n_ = torch.FloatTensor(unroll_n).unsqueeze(1).cuda()\n",
    "\n",
    "        #Critic update\n",
    "        critic_optim.zero_grad()\n",
    "        q_pred = critic(obs_, act_v_)\n",
    "        \n",
    "        q_next_prob = critic_tgt(next_obs_, actor_tgt(next_obs_))\n",
    "        q_next = F.softmax(q_next_prob, dim=1)\n",
    "        q_target = transform_dist(q_next, rew_, GAMMA, unroll_n_, done_)\n",
    "\n",
    "        q_entropy = -F.log_softmax(q_pred, dim=1) * q_target\n",
    "        q_entropy_sum = q_entropy.sum(dim=1)\n",
    "        q_loss = (weights_ * q_entropy_sum).sum()\n",
    "        q_loss.backward()\n",
    "        critic_optim.step()\n",
    "\n",
    "        storage.update_priorities(indices, q_entropy_sum.cpu().detach().numpy())\n",
    "\n",
    "        #Actor update\n",
    "        actor_optim.zero_grad()\n",
    "        q_dist = critic(obs_,actor(obs_))\n",
    "        q_v = -F.softmax(q_dist,dim=1) * torch.arange(V_MIN, V_MAX + V_DIST, V_DIST).cuda()\n",
    "        q_v = q_v.mean(dim=1)\n",
    "\n",
    "        actor_loss = q_v.mean()\n",
    "        actor_loss.backward()\n",
    "        actor_optim.step()\n",
    "\n",
    "        #target update\n",
    "        critic_tgt.alpha_update()\n",
    "        actor_tgt.alpha_update()\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
