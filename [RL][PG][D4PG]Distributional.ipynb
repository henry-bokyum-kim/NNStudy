{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aE6-lCM6oqv3"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYxEy8Q6oqwJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import math\n",
    "\n",
    "class NoiseLinear(nn.Linear):\n",
    "    def __init__(self, in_, out_, val = 0.017, bias = True):\n",
    "        super(NoiseLinear, self).__init__(in_,out_,bias)\n",
    "        self.sigma_weight = nn.Parameter(torch.full((out_, in_), val))\n",
    "        self.register_buffer(\"eps_weight\", torch.zeros(out_, in_))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.full((out_,), val))\n",
    "            self.register_buffer(\"eps_bias\", torch.zeros(out_))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(1 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.eps_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.eps_bias.normal_()\n",
    "            bias = bias + self.sigma_bias * self.eps_bias.data\n",
    "        return F.linear(x, self.weight + self.sigma_weight * self.eps_weight, bias)\n",
    "\n",
    "class targetNet(nn.Module):\n",
    "    def __init__(self, off_net):\n",
    "        super(targetNet, self).__init__()\n",
    "        self.net = copy.deepcopy(off_net)\n",
    "        self.off_net = off_net\n",
    "        \n",
    "    def alpha_update(self, alpha = 0.05):\n",
    "        for off, tgt in zip(self.off_net.parameters(), self.net.parameters()):\n",
    "            tgt.data.copy_(off.data*alpha + tgt.data*(1-alpha))\n",
    "    \n",
    "    def copy_off_net(self):\n",
    "        self.net.load_state_dict(self.off_net.state_dict())\n",
    "    \n",
    "    def forward(self, *x):\n",
    "        return self.net(*x)\n",
    "        \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_, act_n, action_provider, hidden=512):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), act_n),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.action_provider = action_provider\n",
    "        \n",
    "    def get_action(self, act_v):\n",
    "        return self.action_provider(act_v)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DistCritic(nn.Module):\n",
    "    def __init__(self, in_, act_v, atom=51, hidden=512):\n",
    "        super(DistCritic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_, hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.net_out = nn.Sequential(\n",
    "            nn.Linear(hidden + act_n, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), atom)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        return self.net_out(torch.cat([self.net(obs), act], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJrRlHDroqwm"
   },
   "outputs": [],
   "source": [
    "V_MAX = 10\n",
    "V_MIN = -10\n",
    "ATOMS = 51\n",
    "V_DIST = (V_MAX-V_MIN)/(ATOMS-1)\n",
    "\n",
    "def transform_dist(dist, rew, gamma, unroll_n, done):\n",
    "    support_start = V_MIN\n",
    "    \n",
    "    dist = dist.cpu().detach().numpy()\n",
    "    ret = np.zeros_like(dist)\n",
    "    rew = rew.cpu().numpy()\n",
    "    unroll_n = unroll_n.cpu().numpy()\n",
    "    done = done.cpu().numpy()\n",
    "    for atom in range(ATOMS):\n",
    "        support = support_start + atom * V_DIST\n",
    "        next_support = rew + (gamma**unroll_n) * support\n",
    "        next_support[next_support > V_MAX] = V_MAX\n",
    "        next_support[next_support < V_MIN] = V_MIN\n",
    "\n",
    "        indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
    "        l = np.floor(indices).astype(np.int64)\n",
    "        r = np.ceil(indices).astype(np.int64)\n",
    "\n",
    "        eq = l==r\n",
    "        ret[eq, l[eq]] += dist[eq, atom]\n",
    "        neq = l!=r\n",
    "        ret[neq, l[neq]] += dist[neq, atom] * (r - indices)[neq]\n",
    "        ret[neq, r[neq]] += dist[neq, atom] * (indices - l)[neq]\n",
    "\n",
    "        if done.any():\n",
    "            ret[done] = 0.0\n",
    "            next_support = rew[done]\n",
    "            next_support[next_support > V_MAX] = V_MAX\n",
    "            next_support[next_support < V_MIN] = V_MIN\n",
    "\n",
    "            indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
    "            l = np.floor(indices).astype(np.int64)\n",
    "            r = np.ceil(indices).astype(np.int64)\n",
    "\n",
    "            eq = l==r\n",
    "            eq_done = done.copy()\n",
    "            eq_done[done] = eq\n",
    "            if eq_done.any():\n",
    "                ret[eq_done, l[eq]] = 1.0\n",
    "\n",
    "            neq = l!=r\n",
    "            neq_done = done.copy()\n",
    "            neq_done[done] = neq\n",
    "            if neq_done.any():\n",
    "                ret[neq_done, l[neq]] = (r - indices)[neq]\n",
    "                ret[neq_done, r[neq]] = (indices - l)[neq]\n",
    "            \n",
    "    return torch.FloatTensor(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "DxDEeOCfoqwd",
    "outputId": "fae6ec76-623b-4716-da83-54c9a060268d"
   },
   "outputs": [],
   "source": [
    "ACT_LR = 0.001\n",
    "CRT_LR = 0.001\n",
    "GAMMA = 0.99\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     \n",
    "\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "act_n = env.action_space.shape[0]\n",
    "obs_n = env.observation_space.shape[0]\n",
    "\n",
    "actor = Actor(obs_n, act_n, lambda x:x).to(device)\n",
    "actor_tgt = targetNet(actor)\n",
    "actor_optim = optim.Adam(actor.parameters(), ACT_LR)\n",
    "\n",
    "critic = DistCritic(obs_n, act_n, ATOMS).to(device)\n",
    "critic_tgt = targetNet(critic)\n",
    "critic_optim = optim.Adam(critic.parameters(), CRT_LR)\n",
    "\n",
    "ST_SIZE = 50000\n",
    "ST_INIT = 10000\n",
    "ST_DECAY = 5000\n",
    "BATCH = 512\n",
    "\n",
    "storage = Replay(ST_SIZE, True)\n",
    "noise = NoiseMaker(act_n, \"ou\", decay=True)\n",
    "noise.param[\"ou_sig\"] = 0.6\n",
    "noise.param[\"decay\"] = ST_DECAY\n",
    "\n",
    "agent = Agent(env, actor, noise, 1000, device)\n",
    "agent.prepare(2, GAMMA, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Vvt1_9i0oqxQ",
    "outputId": "3377527f-4abf-4fba-dd53-34f61cfae32e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep#   0 count :  132, scaled_rew : -8.90226 total_rew :-133.53390\n",
      "ep#   1 count :  224, scaled_rew : -9.37342 total_rew :-140.60128\n",
      "ep#   2 count :   73, scaled_rew : -7.67079 total_rew :-115.06182\n",
      "ep#   3 count :   65, scaled_rew : -8.52237 total_rew :-127.83549\n",
      "ep#   4 count :  272, scaled_rew : -8.34407 total_rew :-125.16099\n",
      "ep#   5 count :   59, scaled_rew : -7.12473 total_rew :-106.87092\n",
      "ep#   6 count :   62, scaled_rew : -7.52262 total_rew :-112.83929\n",
      "ep#   7 count :   38, scaled_rew : -7.56057 total_rew :-113.40855\n",
      "ep#   8 count :   54, scaled_rew : -6.96447 total_rew :-104.46708\n",
      "ep#   9 count :   57, scaled_rew : -6.99510 total_rew :-104.92650\n",
      "ep#  10 count :   82, scaled_rew : -8.10341 total_rew :-121.55120\n",
      "ep#  11 count :  179, scaled_rew : -7.33822 total_rew :-110.07327\n",
      "ep#  12 count :   58, scaled_rew : -7.53452 total_rew :-113.01778\n",
      "ep#  13 count :  163, scaled_rew : -7.27045 total_rew :-109.05673\n",
      "ep#  14 count :   97, scaled_rew : -7.30116 total_rew :-109.51738\n",
      "ep#  15 count :  211, scaled_rew : -7.02957 total_rew :-105.44351\n",
      "ep#  16 count :   51, scaled_rew : -7.08334 total_rew :-106.25014\n",
      "ep#  17 count :  328, scaled_rew : -8.23660 total_rew :-123.54896\n",
      "ep#  18 count :   45, scaled_rew : -7.79136 total_rew :-116.87037\n",
      "ep#  19 count :   78, scaled_rew : -6.80907 total_rew :-102.13599\n",
      "ep#  20 count :   63, scaled_rew : -6.83288 total_rew :-102.49325\n",
      "ep#  21 count :  206, scaled_rew : -8.41014 total_rew :-126.15217\n",
      "ep#  22 count :  126, scaled_rew : -7.17949 total_rew :-107.69231\n",
      "ep#  23 count :  105, scaled_rew : -7.29252 total_rew :-109.38775\n",
      "ep#  24 count :  219, scaled_rew : -8.20703 total_rew :-123.10543\n",
      "ep#  25 count : 1000, scaled_rew : -1.93523 total_rew :-29.02845\n",
      "ep#  26 count :   59, scaled_rew : -7.21399 total_rew :-108.20987\n",
      "ep#  27 count :   54, scaled_rew : -6.87839 total_rew :-103.17586\n",
      "ep#  28 count :  117, scaled_rew : -8.12983 total_rew :-121.94748\n",
      "ep#  29 count :  323, scaled_rew : -8.75609 total_rew :-131.34138\n",
      "ep#  30 count :  151, scaled_rew : -8.30444 total_rew :-124.56666\n",
      "ep#  31 count :   74, scaled_rew : -6.78671 total_rew :-101.80061\n",
      "ep#  32 count :  151, scaled_rew : -7.91224 total_rew :-118.68355\n",
      "ep#  33 count : 1000, scaled_rew : -1.82768 total_rew :-27.41521\n",
      "ep#  34 count :  667, scaled_rew : -9.24129 total_rew :-138.61934\n",
      "ep#  35 count :   91, scaled_rew : -6.53370 total_rew :-98.00549\n",
      "ep#  36 count :   45, scaled_rew : -7.67662 total_rew :-115.14932\n",
      "ep#  37 count :   62, scaled_rew : -7.11044 total_rew :-106.65662\n",
      "ep#  38 count :   89, scaled_rew : -7.69687 total_rew :-115.45304\n",
      "ep#  39 count :   90, scaled_rew : -7.50292 total_rew :-112.54374\n",
      "ep#  40 count :  116, scaled_rew : -7.73949 total_rew :-116.09231\n",
      "ep#  41 count :   58, scaled_rew : -7.44272 total_rew :-111.64074\n",
      "ep#  42 count :   50, scaled_rew : -7.02705 total_rew :-105.40581\n",
      "ep#  43 count :   82, scaled_rew : -6.42094 total_rew :-96.31407\n",
      "ep#  44 count : 1000, scaled_rew : -1.23412 total_rew :-18.51180\n",
      "ep#  45 count :  236, scaled_rew : -8.86434 total_rew :-132.96506\n",
      "ep#  46 count :   82, scaled_rew : -7.89802 total_rew :-118.47026\n",
      "ep#  47 count :   96, scaled_rew : -6.53060 total_rew :-97.95903\n",
      "ep#  48 count :   63, scaled_rew : -7.74074 total_rew :-116.11105\n",
      "ep#  49 count :  110, scaled_rew : -7.92705 total_rew :-118.90576\n",
      "ep#  50 count :   71, scaled_rew : -7.79476 total_rew :-116.92142\n",
      "ep#  51 count :   89, scaled_rew : -7.44971 total_rew :-111.74560\n",
      "ep#  52 count :   66, scaled_rew : -6.38934 total_rew :-95.84008\n",
      "ep#  53 count :  961, scaled_rew : -8.54225 total_rew :-128.13378\n",
      "ep#  54 count :  130, scaled_rew : -7.66922 total_rew :-115.03837\n",
      "ep#  55 count :  139, scaled_rew : -7.71056 total_rew :-115.65837\n",
      "ep#  56 count :  139, scaled_rew : -7.68239 total_rew :-115.23578\n",
      "ep#  57 count :   86, scaled_rew : -7.50943 total_rew :-112.64138\n",
      "ep#  58 count :  138, scaled_rew : -7.68853 total_rew :-115.32795\n",
      "ep#  59 count :   58, scaled_rew : -7.49705 total_rew :-112.45582\n",
      "ep#  60 count :   55, scaled_rew : -7.71389 total_rew :-115.70839\n",
      "ep#  61 count : 1000, scaled_rew : -6.50370 total_rew :-97.55547\n",
      "ep#  62 count : 1000, scaled_rew : -4.55814 total_rew :-68.37215\n",
      "ep#  63 count : 1000, scaled_rew : -4.96071 total_rew :-74.41066\n",
      "ep#  64 count :  494, scaled_rew : -9.59795 total_rew :-143.96923\n",
      "ep#  65 count :   88, scaled_rew : -8.39168 total_rew :-125.87524\n",
      "ep#  66 count :  238, scaled_rew : -8.40848 total_rew :-126.12727\n",
      "ep#  67 count : 1000, scaled_rew : -1.59822 total_rew :-23.97325\n",
      "ep#  68 count : 1000, scaled_rew : -0.60549 total_rew :-9.08239\n",
      "ep#  69 count :   53, scaled_rew : -7.43131 total_rew :-111.46961\n",
      "ep#  70 count :  151, scaled_rew : -6.68818 total_rew :-100.32274\n",
      "ep#  71 count :  436, scaled_rew : -8.04330 total_rew :-120.64951\n",
      "ep#  72 count :   83, scaled_rew : -7.27334 total_rew :-109.10009\n",
      "ep#  73 count :  102, scaled_rew : -7.42253 total_rew :-111.33793\n",
      "ep#  74 count : 1000, scaled_rew : -5.66158 total_rew :-84.92369\n",
      "ep#  75 count :  188, scaled_rew : -7.22925 total_rew :-108.43877\n",
      "ep#  76 count : 1000, scaled_rew : -0.68371 total_rew :-10.25572\n",
      "ep#  77 count :   85, scaled_rew : -7.21194 total_rew :-108.17905\n",
      "ep#  78 count : 1000, scaled_rew : 2.06101 total_rew :30.91511\n",
      "ep#  79 count :   89, scaled_rew : -6.92657 total_rew :-103.89853\n",
      "ep#  80 count : 1000, scaled_rew : 2.24225 total_rew :33.63375\n",
      "ep#  81 count :  169, scaled_rew : -7.44022 total_rew :-111.60332\n",
      "ep#  82 count : 1000, scaled_rew : 2.78704 total_rew :41.80558\n",
      "ep#  83 count :  103, scaled_rew : -7.17131 total_rew :-107.56959\n",
      "ep#  84 count : 1000, scaled_rew : 2.48209 total_rew :37.23129\n",
      "ep#  85 count :   80, scaled_rew : -6.70281 total_rew :-100.54218\n",
      "ep#  86 count :   94, scaled_rew : -6.95076 total_rew :-104.26138\n",
      "ep#  87 count :   90, scaled_rew : -7.91780 total_rew :-118.76697\n",
      "ep#  88 count :  698, scaled_rew : -4.75642 total_rew :-71.34626\n",
      "ep#  89 count :   72, scaled_rew : -7.35729 total_rew :-110.35928\n",
      "ep#  90 count :  324, scaled_rew : -9.96783 total_rew :-149.51752\n",
      "ep#  91 count :  100, scaled_rew : -8.83931 total_rew :-132.58967\n",
      "ep#  92 count :  102, scaled_rew : -7.42538 total_rew :-111.38065\n",
      "ep#  93 count :   80, scaled_rew : -8.02296 total_rew :-120.34438\n",
      "ep#  94 count :  344, scaled_rew : -6.37263 total_rew :-95.58949\n",
      "ep#  95 count :  121, scaled_rew : -6.32877 total_rew :-94.93149\n",
      "ep#  96 count :  165, scaled_rew : -7.15917 total_rew :-107.38761\n",
      "ep#  97 count :  157, scaled_rew : -6.88201 total_rew :-103.23008\n",
      "ep#  98 count :  555, scaled_rew : -6.45371 total_rew :-96.80568\n",
      "ep#  99 count :  144, scaled_rew : -8.29422 total_rew :-124.41332\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "EPOCH = 2000\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i, step in enumerate(agent.episode(epoch)):\n",
    "        storage.push(step)\n",
    "        if len(storage) < ST_INIT:\n",
    "            continue\n",
    "            \n",
    "        sample, indices, weights = storage.sample(BATCH)\n",
    "        weights_ = torch.FloatTensor(weights).to(device)\n",
    "        obs, act_v, noise_v, act, next_obs, rew, done, etc, unroll_n = list(zip(*sample))\n",
    "        \n",
    "        obs_ = torch.FloatTensor(obs).to(device)\n",
    "        act_v_ = torch.FloatTensor(act_v).to(device)\n",
    "        noise_v_ = torch.FloatTensor(noise_v).to(device)\n",
    "        act_ = torch.LongTensor(act).unsqueeze(1).to(device)\n",
    "        next_obs_ = torch.FloatTensor(next_obs).to(device)\n",
    "        rew_ = torch.FloatTensor(rew).unsqueeze(1).to(device)\n",
    "        done_ = torch.BoolTensor(done).to(device)\n",
    "        unroll_n_ = torch.FloatTensor(unroll_n).unsqueeze(1).to(device)\n",
    "\n",
    "        #Critic update\n",
    "        critic_optim.zero_grad()\n",
    "        q_pred = critic(obs_, noise_v_)\n",
    "        \n",
    "        q_next_prob = critic_tgt(next_obs_, actor_tgt(next_obs_))\n",
    "        q_next = F.softmax(q_next_prob, dim=1)\n",
    "        q_target = transform_dist(q_next, rew_, GAMMA, unroll_n_, done_).to(device)\n",
    "\n",
    "        q_entropy = -F.log_softmax(q_pred, dim=1) * q_target\n",
    "        q_entropy_sum = q_entropy.sum(dim=1)\n",
    "        q_loss = (weights_ * q_entropy_sum).sum()\n",
    "        q_loss.backward()\n",
    "        critic_optim.step()\n",
    "\n",
    "        storage.update_priorities(indices, q_entropy_sum.cpu().detach().numpy())\n",
    "\n",
    "        #Actor update\n",
    "        actor_optim.zero_grad()\n",
    "        \n",
    "        #history decaying\n",
    "        st_decay = torch.exp_(-torch.FloatTensor(len(storage)-indices)/ST_DECAY).unsqueeze(1).to(device)\n",
    "        act_avg = actor(obs_) * (1-st_decay) + act_v_ * st_decay\n",
    "        q_dist = critic(obs_,act_avg)\n",
    "        \n",
    "        q_v = -F.softmax(q_dist,dim=1) * torch.arange(V_MIN, V_MAX + V_DIST, V_DIST).to(device)\n",
    "        q_v = q_v.mean(dim=1)\n",
    "\n",
    "        actor_loss = q_v.mean()\n",
    "        actor_loss.backward()\n",
    "        actor_optim.step()\n",
    "\n",
    "        #target update\n",
    "        critic_tgt.alpha_update()\n",
    "        actor_tgt.alpha_update()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
