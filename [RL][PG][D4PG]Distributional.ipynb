{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aE6-lCM6oqv3",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import namedtuple\n",
        "\n",
        "version = \"1.0.0\"\n",
        "\n",
        "StepInfo = namedtuple(\"StepInfo\", (\"obs\", \"act_v\", \"noise_v\", \"act\", \"last_obs\", \"rew\", \"done\", \"etc\", \"n\"))\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, actor, noise = None, max_step = None, device = None):\n",
        "        self.env = env\n",
        "        if max_step is not None:\n",
        "            self.env._max_episode_steps = max_step\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")            \n",
        "        self.actor = actor\n",
        "        self.noise = noise\n",
        "        self.device = device\n",
        "        \n",
        "        self.wait = -1\n",
        "        self.interval = -1\n",
        "        self.frame = None\n",
        "        \n",
        "        self.step_set = False\n",
        "        self.n_step = 1\n",
        "        self.gamma = 0.99\n",
        "        self.scale_factor = 1\n",
        "        self.bias=0\n",
        "        \n",
        "    def prepare(self, step_unroll, gamma, scale=1, bias=0):\n",
        "        self.step_set = True\n",
        "        self.n_step = step_unroll\n",
        "        self.gamma= gamma\n",
        "        self.scale_factor = scale\n",
        "        self.bias=bias\n",
        "        \n",
        "    def set_renderer(self, rend_wait=0, rend_interval=1, frame = None):\n",
        "        self.wait = rend_wait\n",
        "        self.interval = rend_interval\n",
        "        self.frame = frame\n",
        "        \n",
        "    def reset(self):\n",
        "        self.env.close()\n",
        "        self.env.reset()\n",
        "        \n",
        "    def render(self, epoch):\n",
        "        if self.wait >= 0 and epoch < self.wait:\n",
        "            return\n",
        "        if self.interval >= 0 and epoch % self.interval == 0:\n",
        "            rend = self.env.render(\"rgb_array\")\n",
        "            if self.frame is not None:\n",
        "                self.frame.append(self.env.render(\"rgb_array\"))\n",
        "        \n",
        "    def episode(self, epoch):\n",
        "        assert self.step_set\n",
        "        buffer = []\n",
        "        self.obs = self.env.reset()\n",
        "        self.render(epoch)\n",
        "\n",
        "        total_rew = 0\n",
        "        total_rew_scaled = 0\n",
        "        count = 0\n",
        "        while True:\n",
        "            with torch.no_grad():\n",
        "                act_v = self.actor(torch.FloatTensor([self.obs]).to(self.device)).cpu().squeeze(0).numpy()\n",
        "                if self.noise is not None:\n",
        "                    noise_v = act_v + self.noise.get_noise()\n",
        "                else:\n",
        "                    noise_v = act_v\n",
        "                if self.env.action_space.shape:\n",
        "                    noise_v = noise_v.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                act = self.actor.get_action(noise_v)\n",
        "\n",
        "            next_obs, rew, done, etc = self.env.step(act)\n",
        "            obs = self.obs\n",
        "            self.obs = next_obs\n",
        "            count += 1\n",
        "\n",
        "            total_rew_scaled += rew\n",
        "            rew += self.bias\n",
        "            rew /= self.scale_factor\n",
        "            total_rew += rew\n",
        "\n",
        "            self.render(epoch)\n",
        "\n",
        "            buffer.append(StepInfo(obs, act_v, noise_v, act, next_obs, rew, done, etc, -1))\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "            if len(buffer) < self.n_step:\n",
        "                continue\n",
        "                \n",
        "            yield self.unroll_step(buffer)\n",
        "            buffer.pop(0)\n",
        "            \n",
        "        while len(buffer):\n",
        "            yield self.unroll_step(buffer)\n",
        "            buffer.pop(0)\n",
        "        print(\"ep#%4d\"%epoch, \"count : %4d, scaled_rew : %03.5f total_rew :%03.5f\"%(count, total_rew, total_rew_scaled))\n",
        "        return\n",
        "\n",
        "    def unroll_step(self, buffer):\n",
        "        assert len(buffer)\n",
        "        \n",
        "        rews = list(map(lambda b:b.rew, buffer))\n",
        "        rews.reverse()\n",
        "        rew_sum = 0\n",
        "\n",
        "        for r in rews:\n",
        "            rew_sum*=self.gamma\n",
        "            rew_sum+=r\n",
        "            \n",
        "        done = buffer[-1].done if len(buffer) == self.n_step else True\n",
        "        return StepInfo(buffer[0].obs, buffer[0].act_v, buffer[0].noise_v, buffer[0].act, buffer[-1].last_obs, rew_sum, done, buffer[0].etc, len(buffer))\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class NoiseMaker():\n",
        "    def __init__(self, action_size, n_type = None, param = None, decay = True):\n",
        "        self.action_size = action_size\n",
        "        self.state = np.zeros(action_size, dtype=np.float32)\n",
        "        self.count = 0\n",
        "        self.decay = decay\n",
        "        if n_type is None:\n",
        "            n_type = \"normal\"\n",
        "        self.type = n_type\n",
        "        \n",
        "        if param is None:\n",
        "            self.param = {\n",
        "                \"start\": 0.9,\n",
        "                \"end\":0.02,\n",
        "                \"decay\": 2000\n",
        "            }\n",
        "            if n_type ==\"ou\":\n",
        "                self.param[\"ou_mu\"] = 0.0\n",
        "                self.param[\"ou_th\"] = 0.15\n",
        "                self.param[\"ou_sig\"] = 0.2\n",
        "        else:\n",
        "            self.param = param\n",
        "            \n",
        "    def get_noise(self):\n",
        "        eps = self.param[\"end\"] + (self.param[\"start\"] - self.param[\"end\"]) * math.exp(-1*self.count/ self.param[\"decay\"])\n",
        "        \n",
        "        noise = np.random.normal(size=self.action_size)\n",
        "        if self.type == \"ou\":\n",
        "            self.state += self.param[\"ou_th\"] * (self.param[\"ou_mu\"] - self.state) + self.param[\"ou_sig\"] * noise\n",
        "            noise = self.state\n",
        "        if not self.decay:\n",
        "            eps = 1\n",
        "        self.count += 1\n",
        "            \n",
        "        return noise * eps\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Replay:\n",
        "    def __init__(self, size, prio = False, alph = 0.6, beta = 0.4):\n",
        "        self.memory = collections.deque(maxlen = size)\n",
        "        self.size = size\n",
        "        self.priorities = collections.deque(maxlen = size)\n",
        "        self.prio = prio\n",
        "        self.alph = alph if prio else 0\n",
        "        self.beta = beta if prio else 0\n",
        "        self.count = 0\n",
        "        \n",
        "    def push(self, data):\n",
        "        self.memory.append(data)\n",
        "        self.count += 1\n",
        "        if self.prio:\n",
        "            max_prio = np.array(self.priorities).max() if len(self.priorities) else 1.\n",
        "            self.priorities.append(max_prio)\n",
        "        \n",
        "    def prepare(self, env):\n",
        "        pass\n",
        "        \n",
        "    def sample(self, size):\n",
        "        if self.prio:\n",
        "            probs = np.array(self.priorities, dtype=np.float32)\n",
        "            min_prio = np.array(self.priorities).min()\n",
        "            if min_prio < 1:\n",
        "                probs /= min_prio\n",
        "            probs = probs ** self.alph\n",
        "            probs /= probs.sum()\n",
        "        else:\n",
        "            probs = np.ones(len(self),) / len(self)\n",
        "        \n",
        "        indices = np.random.choice(len(self), size, p=probs)\n",
        "        sample = [self.memory[idx] for idx in indices]\n",
        "        \n",
        "        beta = 1. + (self.beta - 1.) * math.exp(-1 * self.count / self.size)\n",
        "        weights = (len(self) * probs[indices]) ** (-beta)\n",
        "        weights /= weights.sum()\n",
        "        \n",
        "        return sample, indices, weights\n",
        "        \n",
        "    def update_priorities(self, indices, prios):\n",
        "        if not self.prio:\n",
        "            return\n",
        "        prios += 1e-8\n",
        "        for idx, prio in zip(indices,prios):\n",
        "            self.priorities[idx] = prio\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vYxEy8Q6oqwJ",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import math\n",
        "\n",
        "class NoiseLinear(nn.Linear):\n",
        "    def __init__(self, in_, out_, val = 0.017, bias = True):\n",
        "        super(NoiseLinear, self).__init__(in_,out_,bias)\n",
        "        self.sigma_weight = nn.Parameter(torch.full((out_, in_), val))\n",
        "        self.register_buffer(\"eps_weight\", torch.zeros(out_, in_))\n",
        "        if bias:\n",
        "            self.sigma_bias = nn.Parameter(torch.full((out_,), val))\n",
        "            self.register_buffer(\"eps_bias\", torch.zeros(out_))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = math.sqrt(1 / self.in_features)\n",
        "        self.weight.data.uniform_(-std, std)\n",
        "        self.bias.data.uniform_(-std, std)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.eps_weight.normal_()\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            self.eps_bias.normal_()\n",
        "            bias = bias + self.sigma_bias * self.eps_bias.data\n",
        "        return F.linear(x, self.weight + self.sigma_weight * self.eps_weight, bias)\n",
        "\n",
        "class targetNet(nn.Module):\n",
        "    def __init__(self, off_net):\n",
        "        super(targetNet, self).__init__()\n",
        "        self.net = copy.deepcopy(off_net)\n",
        "        self.off_net = off_net\n",
        "        \n",
        "    def alpha_update(self, alpha = 0.05):\n",
        "        for off, tgt in zip(self.off_net.parameters(), self.net.parameters()):\n",
        "            tgt.data.copy_(off.data*alpha + tgt.data*(1-alpha))\n",
        "    \n",
        "    def copy_off_net(self):\n",
        "        self.net.load_state_dict(self.off_net.state_dict())\n",
        "    \n",
        "    def forward(self, *x):\n",
        "        return self.net(*x)\n",
        "        \n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, in_, act_n, action_provider, hidden=512):\n",
        "        super(Actor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, int(hidden/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(hidden/2), act_n),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.action_provider = action_provider\n",
        "        \n",
        "    def get_action(self, act_v):\n",
        "        return self.action_provider(act_v)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DistCritic(nn.Module):\n",
        "    def __init__(self, in_, act_v, atom=51, hidden=512):\n",
        "        super(DistCritic, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_, hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.net_out = nn.Sequential(\n",
        "            nn.Linear(hidden + act_n, int(hidden/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(hidden/2), atom)\n",
        "        )\n",
        "    \n",
        "    def forward(self, obs, act):\n",
        "        return self.net_out(torch.cat([self.net(obs), act], dim=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LJrRlHDroqwm",
        "colab": {}
      },
      "source": [
        "V_MAX = 10\n",
        "V_MIN = -10\n",
        "ATOMS = 51\n",
        "V_DIST = (V_MAX-V_MIN)/(ATOMS-1)\n",
        "\n",
        "def transform_dist(dist, rew, gamma, unroll_n, done):\n",
        "    support_start = V_MIN\n",
        "    \n",
        "    dist = dist.cpu().detach().numpy()\n",
        "    ret = np.zeros_like(dist)\n",
        "    rew = rew.cpu().numpy()\n",
        "    unroll_n = unroll_n.cpu().numpy()\n",
        "    done = done.cpu().numpy()\n",
        "    for atom in range(ATOMS):\n",
        "        support = support_start + atom * V_DIST\n",
        "        next_support = rew + (gamma**unroll_n) * support\n",
        "        next_support[next_support > V_MAX] = V_MAX\n",
        "        next_support[next_support < V_MIN] = V_MIN\n",
        "\n",
        "        indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
        "        l = np.floor(indices).astype(np.int64)\n",
        "        r = np.ceil(indices).astype(np.int64)\n",
        "\n",
        "        eq = l==r\n",
        "        ret[eq, l[eq]] += dist[eq, atom]\n",
        "        neq = l!=r\n",
        "        ret[neq, l[neq]] += dist[neq, atom] * (r - indices)[neq]\n",
        "        ret[neq, r[neq]] += dist[neq, atom] * (indices - l)[neq]\n",
        "\n",
        "        if done.any():\n",
        "            ret[done] = 0.0\n",
        "            next_support = rew[done]\n",
        "            next_support[next_support > V_MAX] = V_MAX\n",
        "            next_support[next_support < V_MIN] = V_MIN\n",
        "\n",
        "            indices = ((next_support - V_MIN) / V_DIST).squeeze()\n",
        "            l = np.floor(indices).astype(np.int64)\n",
        "            r = np.ceil(indices).astype(np.int64)\n",
        "\n",
        "            eq = l==r\n",
        "            eq_done = done.copy()\n",
        "            eq_done[done] = eq\n",
        "            if eq_done.any():\n",
        "                ret[eq_done, l[eq]] = 1.0\n",
        "\n",
        "            neq = l!=r\n",
        "            neq_done = done.copy()\n",
        "            neq_done[done] = neq\n",
        "            if neq_done.any():\n",
        "                ret[neq_done, l[neq]] = (r - indices)[neq]\n",
        "                ret[neq_done, r[neq]] = (indices - l)[neq]\n",
        "            \n",
        "    return torch.FloatTensor(ret)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DxDEeOCfoqwd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fae6ec76-623b-4716-da83-54c9a060268d"
      },
      "source": [
        "ACT_LR = 0.001\n",
        "CRT_LR = 0.001\n",
        "GAMMA = 0.99\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     \n",
        "\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "act_n = env.action_space.shape[0]\n",
        "obs_n = env.observation_space.shape[0]\n",
        "\n",
        "actor = Actor(obs_n, act_n, lambda x:x).to(device)\n",
        "actor_tgt = targetNet(actor)\n",
        "actor_optim = optim.Adam(actor.parameters(), ACT_LR)\n",
        "\n",
        "critic = DistCritic(obs_n, act_n, ATOMS).to(device)\n",
        "critic_tgt = targetNet(critic)\n",
        "critic_optim = optim.Adam(critic.parameters(), CRT_LR)\n",
        "\n",
        "ST_SIZE = 50000\n",
        "ST_INIT = 10000\n",
        "ST_DECAY = 5000\n",
        "BATCH = 512\n",
        "\n",
        "storage = Replay(ST_SIZE, True)\n",
        "noise = NoiseMaker(act_n, \"ou\", decay=True)\n",
        "noise.param[\"ou_sig\"] = 0.6\n",
        "noise.param[\"decay\"] = ST_DECAY\n",
        "\n",
        "agent = Agent(env, actor, noise, 1000, device)\n",
        "agent.prepare(2, GAMMA, 100)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vvt1_9i0oqxQ",
        "outputId": "3377527f-4abf-4fba-dd53-34f61cfae32e",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "EPOCH = 2000\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    for i, step in enumerate(agent.episode(epoch)):\n",
        "        storage.push(step)\n",
        "        if len(storage) < ST_INIT:\n",
        "            continue\n",
        "            \n",
        "        sample, indices, weights = storage.sample(BATCH)\n",
        "        weights_ = torch.FloatTensor(weights).to(device)\n",
        "        obs, act_v, noise_v, act, next_obs, rew, done, etc, unroll_n = list(zip(*sample))\n",
        "        \n",
        "        obs_ = torch.FloatTensor(obs).to(device)\n",
        "        act_v_ = torch.FloatTensor(act_v).to(device)\n",
        "        noise_v_ = torch.FloatTensor(noise_v).to(device)\n",
        "        act_ = torch.LongTensor(act).unsqueeze(1).to(device)\n",
        "        next_obs_ = torch.FloatTensor(next_obs).to(device)\n",
        "        rew_ = torch.FloatTensor(rew).unsqueeze(1).to(device)\n",
        "        done_ = torch.BoolTensor(done).to(device)\n",
        "        unroll_n_ = torch.FloatTensor(unroll_n).unsqueeze(1).to(device)\n",
        "\n",
        "        #Critic update\n",
        "        critic_optim.zero_grad()\n",
        "        q_pred = critic(obs_, noise_v_)\n",
        "        \n",
        "        q_next_prob = critic_tgt(next_obs_, actor_tgt(next_obs_))\n",
        "        q_next = F.softmax(q_next_prob, dim=1)\n",
        "        q_target = transform_dist(q_next, rew_, GAMMA, unroll_n_, done_).to(device)\n",
        "\n",
        "        q_entropy = -F.log_softmax(q_pred, dim=1) * q_target\n",
        "        q_entropy_sum = q_entropy.sum(dim=1)\n",
        "        q_loss = (weights_ * q_entropy_sum).sum()\n",
        "        q_loss.backward()\n",
        "        critic_optim.step()\n",
        "\n",
        "        storage.update_priorities(indices, q_entropy_sum.cpu().detach().numpy())\n",
        "\n",
        "        #Actor update\n",
        "        actor_optim.zero_grad()\n",
        "        \n",
        "        #history decaying\n",
        "        st_decay = torch.exp_(-torch.FloatTensor(ST_DECAY-indices)/ST_DECAY).unsqueeze(1).to(device)\n",
        "        act_avg = actor(obs_) * (1-st_decay) + act_v_ * st_decay\n",
        "        q_dist = critic(obs_,act_avg)\n",
        "        \n",
        "        q_v = -F.softmax(q_dist,dim=1) * torch.arange(V_MIN, V_MAX + V_DIST, V_DIST).to(device)\n",
        "        q_v = q_v.mean(dim=1)\n",
        "\n",
        "        actor_loss = q_v.mean()\n",
        "        actor_loss.backward()\n",
        "        actor_optim.step()\n",
        "\n",
        "        #target update\n",
        "        critic_tgt.alpha_update()\n",
        "        actor_tgt.alpha_update()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep#   0 count :   58, scaled_rew : -1.15358 total_rew :-115.35814\n",
            "ep#   1 count :   69, scaled_rew : -1.14443 total_rew :-114.44262\n",
            "ep#   2 count :   60, scaled_rew : -1.10631 total_rew :-110.63132\n",
            "ep#   3 count :  179, scaled_rew : -1.22192 total_rew :-122.19166\n",
            "ep#   4 count :   52, scaled_rew : -1.17310 total_rew :-117.31012\n",
            "ep#   5 count :  315, scaled_rew : -1.12949 total_rew :-112.94932\n",
            "ep#   6 count :  237, scaled_rew : -1.33407 total_rew :-133.40662\n",
            "ep#   7 count :   34, scaled_rew : -1.11974 total_rew :-111.97435\n",
            "ep#   8 count :   52, scaled_rew : -1.14469 total_rew :-114.46915\n",
            "ep#   9 count :   44, scaled_rew : -1.10202 total_rew :-110.20220\n",
            "ep#  10 count :   57, scaled_rew : -1.05150 total_rew :-105.15009\n",
            "ep#  11 count :   77, scaled_rew : -1.15603 total_rew :-115.60322\n",
            "ep#  12 count :   62, scaled_rew : -1.12374 total_rew :-112.37370\n",
            "ep#  13 count :  236, scaled_rew : -1.23240 total_rew :-123.24035\n",
            "ep#  14 count :   50, scaled_rew : -1.14161 total_rew :-114.16067\n",
            "ep#  15 count :   81, scaled_rew : -1.02474 total_rew :-102.47430\n",
            "ep#  16 count :   78, scaled_rew : -1.15773 total_rew :-115.77262\n",
            "ep#  17 count :  631, scaled_rew : -1.13459 total_rew :-113.45928\n",
            "ep#  18 count :   67, scaled_rew : -1.09525 total_rew :-109.52499\n",
            "ep#  19 count :  509, scaled_rew : -1.43967 total_rew :-143.96676\n",
            "ep#  20 count :  450, scaled_rew : -1.30709 total_rew :-130.70929\n",
            "ep#  21 count :   78, scaled_rew : -0.98045 total_rew :-98.04542\n",
            "ep#  22 count :   54, scaled_rew : -1.17643 total_rew :-117.64338\n",
            "ep#  23 count :   91, scaled_rew : -1.17161 total_rew :-117.16108\n",
            "ep#  24 count :  507, scaled_rew : -1.39094 total_rew :-139.09439\n",
            "ep#  25 count :  466, scaled_rew : -1.23948 total_rew :-123.94791\n",
            "ep#  26 count :   43, scaled_rew : -1.01184 total_rew :-101.18389\n",
            "ep#  27 count :   97, scaled_rew : -0.98118 total_rew :-98.11803\n",
            "ep#  28 count :  526, scaled_rew : -1.34436 total_rew :-134.43613\n",
            "ep#  29 count : 1000, scaled_rew : -0.38367 total_rew :-38.36671\n",
            "ep#  30 count : 1000, scaled_rew : -0.13830 total_rew :-13.82960\n",
            "ep#  31 count :  110, scaled_rew : -1.01672 total_rew :-101.67150\n",
            "ep#  32 count : 1000, scaled_rew : -0.18144 total_rew :-18.14398\n",
            "ep#  33 count : 1000, scaled_rew : -0.19134 total_rew :-19.13438\n",
            "ep#  34 count :  900, scaled_rew : -1.22955 total_rew :-122.95466\n",
            "ep#  35 count :   47, scaled_rew : -1.07985 total_rew :-107.98505\n",
            "ep#  36 count :   44, scaled_rew : -1.08165 total_rew :-108.16472\n",
            "ep#  37 count :   66, scaled_rew : -1.03004 total_rew :-103.00406\n",
            "ep#  38 count :   82, scaled_rew : -1.02020 total_rew :-102.01989\n",
            "ep#  39 count :   46, scaled_rew : -1.19345 total_rew :-119.34544\n",
            "ep#  40 count :  105, scaled_rew : -1.32281 total_rew :-132.28142\n",
            "ep#  41 count :   60, scaled_rew : -1.01252 total_rew :-101.25230\n",
            "ep#  42 count :   56, scaled_rew : -1.01506 total_rew :-101.50619\n",
            "ep#  43 count :   45, scaled_rew : -1.07963 total_rew :-107.96348\n",
            "ep#  44 count :   47, scaled_rew : -1.07158 total_rew :-107.15781\n",
            "ep#  45 count :   46, scaled_rew : -1.07859 total_rew :-107.85882\n",
            "ep#  46 count :   46, scaled_rew : -1.07895 total_rew :-107.89490\n",
            "ep#  47 count :   46, scaled_rew : -1.06799 total_rew :-106.79856\n",
            "ep#  48 count :   64, scaled_rew : -1.02088 total_rew :-102.08759\n",
            "ep#  49 count :  123, scaled_rew : -1.36053 total_rew :-136.05300\n",
            "ep#  50 count :   46, scaled_rew : -1.06729 total_rew :-106.72948\n",
            "ep#  51 count :   46, scaled_rew : -1.05943 total_rew :-105.94252\n",
            "ep#  52 count :   51, scaled_rew : -1.05699 total_rew :-105.69858\n",
            "ep#  53 count :   62, scaled_rew : -1.03677 total_rew :-103.67737\n",
            "ep#  54 count :   84, scaled_rew : -1.02130 total_rew :-102.13031\n",
            "ep#  55 count :   46, scaled_rew : -1.07227 total_rew :-107.22697\n",
            "ep#  56 count :   67, scaled_rew : -1.02323 total_rew :-102.32345\n",
            "ep#  57 count :   48, scaled_rew : -1.05305 total_rew :-105.30468\n",
            "ep#  58 count :   46, scaled_rew : -1.06662 total_rew :-106.66155\n",
            "ep#  59 count :   46, scaled_rew : -1.06215 total_rew :-106.21457\n",
            "ep#  60 count :   46, scaled_rew : -1.06333 total_rew :-106.33324\n",
            "ep#  61 count :   47, scaled_rew : -1.07633 total_rew :-107.63342\n",
            "ep#  62 count :   71, scaled_rew : -1.04398 total_rew :-104.39838\n",
            "ep#  63 count :   90, scaled_rew : -1.03537 total_rew :-103.53654\n",
            "ep#  64 count :   57, scaled_rew : -1.03544 total_rew :-103.54390\n",
            "ep#  65 count :   47, scaled_rew : -1.07281 total_rew :-107.28050\n",
            "ep#  66 count :   47, scaled_rew : -1.06459 total_rew :-106.45942\n",
            "ep#  67 count :   46, scaled_rew : -1.06620 total_rew :-106.62016\n",
            "ep#  68 count :   68, scaled_rew : -1.02014 total_rew :-102.01410\n",
            "ep#  69 count :   61, scaled_rew : -1.02694 total_rew :-102.69384\n",
            "ep#  70 count :   45, scaled_rew : -1.06420 total_rew :-106.42027\n",
            "ep#  71 count :   63, scaled_rew : -1.02186 total_rew :-102.18612\n",
            "ep#  72 count :   46, scaled_rew : -1.06406 total_rew :-106.40626\n",
            "ep#  73 count :   46, scaled_rew : -1.06191 total_rew :-106.19120\n",
            "ep#  74 count :   47, scaled_rew : -1.05990 total_rew :-105.98968\n",
            "ep#  75 count :   47, scaled_rew : -1.06244 total_rew :-106.24417\n",
            "ep#  76 count :   58, scaled_rew : -1.03322 total_rew :-103.32238\n",
            "ep#  77 count :   46, scaled_rew : -1.06591 total_rew :-106.59130\n",
            "ep#  78 count :   46, scaled_rew : -1.06651 total_rew :-106.65073\n",
            "ep#  79 count :   47, scaled_rew : -1.06203 total_rew :-106.20278\n",
            "ep#  80 count :   53, scaled_rew : -1.05288 total_rew :-105.28784\n",
            "ep#  81 count :   51, scaled_rew : -1.02917 total_rew :-102.91658\n",
            "ep#  82 count :   59, scaled_rew : -1.02981 total_rew :-102.98149\n",
            "ep#  83 count :   57, scaled_rew : -1.03158 total_rew :-103.15824\n",
            "ep#  84 count :  456, scaled_rew : -1.47385 total_rew :-147.38479\n",
            "ep#  85 count :   62, scaled_rew : -1.12363 total_rew :-112.36281\n",
            "ep#  86 count :   74, scaled_rew : -1.14737 total_rew :-114.73745\n",
            "ep#  87 count :   60, scaled_rew : -1.13761 total_rew :-113.76068\n",
            "ep#  88 count :   61, scaled_rew : -1.14056 total_rew :-114.05568\n",
            "ep#  89 count :   59, scaled_rew : -1.13883 total_rew :-113.88296\n",
            "ep#  90 count :   58, scaled_rew : -1.12580 total_rew :-112.57971\n",
            "ep#  91 count :   57, scaled_rew : -1.12222 total_rew :-112.22162\n",
            "ep#  92 count :   73, scaled_rew : -1.03125 total_rew :-103.12494\n",
            "ep#  93 count :   63, scaled_rew : -1.05088 total_rew :-105.08782\n",
            "ep#  94 count :   66, scaled_rew : -1.04668 total_rew :-104.66828\n",
            "ep#  95 count :   76, scaled_rew : -1.02687 total_rew :-102.68740\n",
            "ep#  96 count :   62, scaled_rew : -1.02650 total_rew :-102.65017\n",
            "ep#  97 count :   51, scaled_rew : -1.04689 total_rew :-104.68941\n",
            "ep#  98 count :   64, scaled_rew : -1.03819 total_rew :-103.81906\n",
            "ep#  99 count :   67, scaled_rew : -1.04093 total_rew :-104.09254\n",
            "ep# 100 count :   86, scaled_rew : -1.08089 total_rew :-108.08941\n",
            "ep# 101 count :   60, scaled_rew : -1.03731 total_rew :-103.73105\n",
            "ep# 102 count :   86, scaled_rew : -1.03246 total_rew :-103.24551\n",
            "ep# 103 count :   68, scaled_rew : -1.02343 total_rew :-102.34300\n",
            "ep# 104 count :   47, scaled_rew : -1.05963 total_rew :-105.96288\n",
            "ep# 105 count :   59, scaled_rew : -1.03922 total_rew :-103.92217\n",
            "ep# 106 count :   62, scaled_rew : -1.04338 total_rew :-104.33813\n",
            "ep# 107 count :   56, scaled_rew : -1.04679 total_rew :-104.67949\n",
            "ep# 108 count :   60, scaled_rew : -1.05081 total_rew :-105.08078\n",
            "ep# 109 count :   58, scaled_rew : -1.04887 total_rew :-104.88710\n",
            "ep# 110 count :   51, scaled_rew : -1.05928 total_rew :-105.92848\n",
            "ep# 111 count :   56, scaled_rew : -1.04397 total_rew :-104.39664\n",
            "ep# 112 count :   60, scaled_rew : -1.03413 total_rew :-103.41260\n",
            "ep# 113 count :   49, scaled_rew : -1.05878 total_rew :-105.87775\n",
            "ep# 114 count :   57, scaled_rew : -1.06790 total_rew :-106.79002\n",
            "ep# 115 count :   76, scaled_rew : -1.08343 total_rew :-108.34329\n",
            "ep# 116 count :   58, scaled_rew : -1.05620 total_rew :-105.62047\n",
            "ep# 117 count :   90, scaled_rew : -1.06712 total_rew :-106.71164\n",
            "ep# 118 count :  163, scaled_rew : -1.46651 total_rew :-146.65119\n",
            "ep# 119 count :   49, scaled_rew : -1.07647 total_rew :-107.64667\n",
            "ep# 120 count :   47, scaled_rew : -1.07181 total_rew :-107.18143\n",
            "ep# 121 count :   69, scaled_rew : -1.04634 total_rew :-104.63387\n",
            "ep# 122 count :   48, scaled_rew : -1.02331 total_rew :-102.33145\n",
            "ep# 123 count :   47, scaled_rew : -1.03458 total_rew :-103.45827\n",
            "ep# 124 count :   52, scaled_rew : -1.03730 total_rew :-103.72953\n",
            "ep# 125 count :   50, scaled_rew : -1.08657 total_rew :-108.65657\n",
            "ep# 126 count :   71, scaled_rew : -1.05134 total_rew :-105.13372\n",
            "ep# 127 count :   47, scaled_rew : -1.05786 total_rew :-105.78560\n",
            "ep# 128 count :   77, scaled_rew : -1.25783 total_rew :-125.78258\n",
            "ep# 129 count :   49, scaled_rew : -1.11188 total_rew :-111.18811\n",
            "ep# 130 count :   60, scaled_rew : -1.20839 total_rew :-120.83880\n",
            "ep# 131 count :   55, scaled_rew : -1.21566 total_rew :-121.56585\n",
            "ep# 132 count :   37, scaled_rew : -1.13746 total_rew :-113.74596\n",
            "ep# 133 count :   37, scaled_rew : -1.12779 total_rew :-112.77908\n",
            "ep# 134 count :   97, scaled_rew : -1.09033 total_rew :-109.03280\n",
            "ep# 135 count :   48, scaled_rew : -1.08454 total_rew :-108.45368\n",
            "ep# 136 count :   81, scaled_rew : -1.10797 total_rew :-110.79689\n",
            "ep# 137 count :   47, scaled_rew : -1.06937 total_rew :-106.93746\n",
            "ep# 138 count :   69, scaled_rew : -1.04040 total_rew :-104.03999\n",
            "ep# 139 count :  100, scaled_rew : -1.28021 total_rew :-128.02099\n",
            "ep# 140 count :   57, scaled_rew : -1.03760 total_rew :-103.76006\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}