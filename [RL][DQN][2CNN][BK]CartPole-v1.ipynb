{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "[RL][DQN][BK]CartPole-v1.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUoEtudHeatf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "4032e5fd-7498-4fe7-e474-d04cde898947"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40)\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400,900),)\n",
        "display.start()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1025'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1025'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMgVj7j_eRJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v0').unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJSd5U1deRJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc43z3zneRJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        # self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        # self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(w))\n",
        "        convh = conv2d_size_out(conv2d_size_out(h))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        hidden_size = 256\n",
        "        self.head = nn.Linear(linear_input_size, hidden_size)\n",
        "        self.last = nn.Linear(hidden_size ,outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        #x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.head(x.view(x.size(0), -1))\n",
        "        return self.last(F.relu(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnzk53ZCeRJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "17b0bd50-12e6-4b9b-fd1b-331652ed3896"
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.3)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # Strip off the edges, so that we have a square image centered on a cart\n",
        "    #screen = screen[:, :, slice_range]\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACICAYAAAD+r7D/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzcwK6v8B1v/Iual4A4IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL9JVL7leRJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.95\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 2000\n",
        "TARGET_UPDATE = 10\n",
        "LR = 0.0008\n",
        "\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "#optimizer = optim.RMSprop(policy_net.parameters(), lr=LR)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        ipythondisplay.display(plt.gcf())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vb8dimneRJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.ones(BATCH_SIZE, device=device)*-1000\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    #loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # for param in policy_net.parameters():\n",
        "    #     param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CvT7a9_meRJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "a3a38271-c2d4-4a2b-e222-0339f0b6be24"
      },
      "source": [
        "num_episodes = 1000\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            if i_episode%20 == 0:\n",
        "                print(\"[%3d]\"% i_episode, end=\"\")\n",
        "            #plot_durations()\n",
        "            print(\"%3d\"%t, end=\" \")\n",
        "            if i_episode%20 == 19:\n",
        "                print()\n",
        "            break\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0] 13  34  12  14  15  15  35  33  16  13  29  10  11  19  11  41  16  23  17  15 \n",
            "[ 20]  9  34   7  13  14  35  15  15  12  58  26  66  14  32  63  18  11  32  16  19 \n",
            "[ 40] 14  32  49  53  14  11  15  46  17  32  52  60  43  24  29  17  11  17  30  32 \n",
            "[ 60] 90  16  48  62  45  50  14  10  40  69  42  33  26  36  43  13  62  42  47  34 \n",
            "[ 80] 61  11  30  49  26  44  38  23  81  55  31  21  38  15  60  43  18  27  53  68 \n",
            "[100] 23  36  32  34  30  34  32  75  43  55  70  13  40  54  56  38  14  31  77  25 \n",
            "[120] 45  58  73  74  47  17  11  97  87  43  65  49  66  43  62  65  22  81  54  86 \n",
            "[140] 70  52  26  20  22  49  40  10  72  59  38  15  41  33  29  47  75  35  87  19 \n",
            "[160] 15  11  19  36  22  25  15  15  13  34  37  66  39  32 109  12  34  85  60  51 \n",
            "[180] 93  37  27  69  34  35  78  48  26  47  44  26  16  53  56  42  47  51  35  58 \n",
            "[200] 62  33  37 107  14  11  23  75  20  56  23 106  91  12  55  26  31  78  61  31 \n",
            "[220] 25  80  34  60   8 109  21  30  28  42  15  38  27  14  26  77  26  49  71  45 \n",
            "[240] 21  48  28  67  45  19  51  70  59  49  23  54  37  25  35  82 105  42  14  86 \n",
            "[260] 16  61  44  67  81 125  70 148  67  75  32  64  36  58  63  72  72 137  14  54 \n",
            "[280] 83  55  18  15 103  44  23  38   9   9  61  32  42  33  14  29 155  52  73 111 \n",
            "[300] 86   9  12  92 106  49  84  80  42  73  52  75  70  63  72  46  51  52  92  43 \n",
            "[320] 65  84  82  74  28  20  16  70  88  70  44  24  42  25 100  30  46  86  99  21 \n",
            "[340] 10  39  46  16  34  51  78  61  43  59  31  34  33 100  24  41  55  52  15  45 \n",
            "[360]119  18  69  49  22  65  61  26  75  14  85  34  30 103  18  43  12  85  66  11 \n",
            "[380] 69  55  39  40  21  16  11  11  29  28  24 106  21  15  26  25  55 137  47  39 \n",
            "[400] 14  57  78  80  26  74  68  53  91  94  84  29 103  90  78  33  58  40  97 112 \n",
            "[420]125  24 108  47  24  15  31  67  20  36  69  61  48 101  74  69  39  64  57  49 \n",
            "[440] 49  49  30 102  67  20  67  90  47  68  77 124 113  23 128  56  89  93 111 101 \n",
            "[460] 40  56  81  21  60  67  62  49 100  32  87  43  49  89  65 109  84  89  25  31 \n",
            "[480] 56  10  56  45  33  62  13 109  81  25  88  85 136 108  54  95 114  39  93  54 \n",
            "[500]102  51  96  18  23  42  19  86  86  62  55  21  60 109  26  33 118  65  49  33 \n",
            "[520] 45  10  19  19  73 134  17  29 150  22  35  24  24 105 106 107  65 102  26  12 \n",
            "[540] 25  14 164  43  82 104  95  82   9  81  31  87  87  11  66  10  39  22 120  31 \n",
            "[560] 77  68  22  53 127 102  29  29  84  88  71  25  34  63  97  39  56  76  64  95 \n",
            "[580] 92  66 124  54 158  27 101  34  36  18  34  71  85  38  19  18  31  30  10  37 \n",
            "[600] 13  14  18  13  49  52  47 103  27  44  42  84  58 107  27  19  37  90  59  26 \n",
            "[620] 33  22  37  54  50  74 114  56  75  32  24  31  17  12  62  40  29  29  14  80 \n",
            "[640] 27  23  26  28  84  46  30  55  74  75 169  42  25  60  70  28  39  69  64  53 \n",
            "[660] 29  94 "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}