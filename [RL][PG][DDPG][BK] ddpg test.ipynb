{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = namedtuple(\"step\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, size):\n",
    "        self.memory = collections.deque(maxlen = size)\n",
    "        \n",
    "    def push(self, data):\n",
    "        self.memory.append(data)\n",
    "        \n",
    "    def prepare(self, env):\n",
    "        pass\n",
    "        \n",
    "    def sample(self, size):\n",
    "        if len(self.memory) >= size:\n",
    "            return random.sample(self.memory, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_n, action_n, hidden = 256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_n, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), action_n),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_n, action_n, hidden =256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_n, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden+action_n, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, act):\n",
    "        temp = self.net(state)\n",
    "        return self.out(torch.cat([temp, act], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "GAME_NAME = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(GAME_NAME)\n",
    "obs_n = env.observation_space.shape[0]\n",
    "act_n = env.action_space.n\n",
    "\n",
    "LR = 0.001\n",
    "TAU = 0.005\n",
    "GAMMA = 0.99\n",
    "\n",
    "actor = Actor(obs_n, act_n)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr = LR)\n",
    "actor_tgt = Actor(obs_n, act_n)\n",
    "actor_tgt.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic = Critic(obs_n, act_n)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr = LR)\n",
    "critic_tgt = Critic(obs_n, act_n)\n",
    "critic_tgt.load_state_dict(critic.state_dict())\n",
    "\n",
    "MAX_MEMORY = 20000\n",
    "MEM_INIT = 2000\n",
    "BATCH = 256\n",
    "storage = Replay(MAX_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 count 500\n",
      "epoch 1 count 500\n",
      "epoch 2 count 500\n",
      "epoch 3 count 500\n",
      "epoch 4 count 274\n",
      "epoch 5 count 312\n",
      "epoch 6 count 97\n",
      "epoch 7 count 155\n",
      "epoch 8 count 141\n",
      "epoch 9 count 112\n",
      "epoch 10 count 128\n",
      "epoch 11 count 118\n",
      "epoch 12 count 107\n",
      "epoch 13 count 190\n",
      "epoch 14 count 131\n",
      "epoch 15 count 194\n",
      "epoch 16 count 148\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    obs = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    count = 0\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            act_v = actor(torch.FloatTensor(obs)).numpy()\n",
    "            noise = np.random.random(act_n)/(epoch+1)\n",
    "            act_v += noise\n",
    "            act = act_v.argmax().item()\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(act)\n",
    "        env.render()\n",
    "        count += 1\n",
    "        \n",
    "        storage.push(step(obs, act_v, next_obs, rew, done))\n",
    "        obs = next_obs\n",
    "        \n",
    "        sample = storage.sample(BATCH)\n",
    "        if sample:\n",
    "            sample = step(*zip(*sample))\n",
    "            \n",
    "            states = torch.FloatTensor(sample.state)\n",
    "            actions = torch.FloatTensor(sample.action)\n",
    "            next_states = torch.FloatTensor(sample.next_state)\n",
    "            rewards = torch.FloatTensor(sample.reward).unsqueeze(-1)\n",
    "            dones = torch.BoolTensor(sample.done).unsqueeze(-1)\n",
    "            \n",
    "            # critic learning\n",
    "            critic_optim.zero_grad()\n",
    "            q_pred = critic(states, actions)\n",
    "            \n",
    "            next_action_v = actor_tgt(next_states)\n",
    "            q_next = critic_tgt(next_states, next_action_v)\n",
    "            q_next[dones] = 0\n",
    "            q_target = rewards + GAMMA * q_next\n",
    "            \n",
    "            critic_loss = F.mse_loss(q_pred, q_target.detach())\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "            \n",
    "            # actor learning\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss = -critic(states, actor(states))\n",
    "            actor_loss = actor_loss.mean()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "            \n",
    "            # tgt soft update\n",
    "            for tgt, real  in zip(actor_tgt.parameters(), actor.parameters()):\n",
    "                tgt.data.copy_(TAU*real.data + (1-TAU)*tgt.data)\n",
    "                \n",
    "            for tgt, real  in zip(critic_tgt.parameters(),critic.parameters()):\n",
    "                tgt.data.copy_(TAU*real.data + (1-TAU)*tgt.data)\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "    print(\"epoch %d count %d\"%(epoch, count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
