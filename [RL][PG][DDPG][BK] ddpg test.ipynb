{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = namedtuple(\"step\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, size):\n",
    "        self.memory = collections.deque(maxlen = size)\n",
    "        \n",
    "    def push(self, data):\n",
    "        self.memory.append(data)\n",
    "        \n",
    "    def prepare(self, env):\n",
    "        pass\n",
    "        \n",
    "    def sample(self, size):\n",
    "        if len(self.memory) >= size:\n",
    "            return random.sample(self.memory, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_n, action_n, hidden = 256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_n, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), action_n),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_n, action_n, hidden =256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_n, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden+action_n, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, act):\n",
    "        temp = self.net(state)\n",
    "        return self.out(torch.cat([temp, act], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "GAME_NAME = \"BipedalWalker-v2\"\n",
    "\n",
    "env = gym.make(GAME_NAME)\n",
    "env._max_episode_steps = 400\n",
    "obs_n = env.observation_space.shape[0]\n",
    "act_n = env.action_space.shape[0]\n",
    "\n",
    "LR_ACT = 0.0001\n",
    "LR_CRT = 0.0005\n",
    "TAU = 0.0005\n",
    "GAMMA = 0.999\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 20000\n",
    "\n",
    "actor = Actor(obs_n, act_n).cuda()\n",
    "actor_optim = optim.Adam(actor.parameters(), lr = LR_ACT)\n",
    "actor_tgt = Actor(obs_n, act_n).cuda()\n",
    "actor_tgt.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic = Critic(obs_n, act_n).cuda()\n",
    "critic_optim = optim.Adam(critic.parameters(), lr = LR_CRT)\n",
    "critic_tgt = Critic(obs_n, act_n).cuda()\n",
    "critic_tgt.load_state_dict(critic.state_dict())\n",
    "\n",
    "MAX_MEMORY = 20000\n",
    "MEM_INIT = 2000\n",
    "BATCH = 256\n",
    "storage = Replay(MAX_MEMORY)\n",
    "step_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 count 400 reward -46.69813\n",
      "epoch 1 count 400 reward -46.55329\n",
      "epoch 2 count 400 reward -46.65319\n",
      "epoch 3 count 400 reward -46.13652\n",
      "epoch 4 count 400 reward -46.55728\n",
      "epoch 5 count 400 reward -46.57750\n",
      "epoch 6 count 400 reward -46.60644\n",
      "epoch 7 count 400 reward -46.68356\n",
      "epoch 8 count 400 reward -46.64587\n",
      "epoch 9 count 400 reward -46.60472\n",
      "epoch 10 count 400 reward -46.16692\n",
      "epoch 11 count 400 reward -46.54889\n",
      "epoch 12 count 400 reward -46.41477\n",
      "epoch 13 count 400 reward -46.57433\n",
      "epoch 14 count 400 reward -46.63324\n",
      "epoch 15 count 400 reward -46.56693\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    obs = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    count = 0\n",
    "    total_rew = 0 \n",
    "    while True:\n",
    "        eps = EPS_END + (EPS_START- EPS_END) * math.exp(-1* step_count / EPS_DECAY)\n",
    "        with torch.no_grad():\n",
    "            act_v = actor(torch.FloatTensor(obs).cuda()).cpu().numpy()\n",
    "            noise = (np.random.random(act_n) - 0.5) * eps\n",
    "            act_v += noise\n",
    "            act_v = act_v.clip(-1,1)\n",
    "\n",
    "        next_obs, rew, done, _ = env.step(act_v)\n",
    "        \n",
    "        #print(noise, act_v, rew)\n",
    "        env.render()\n",
    "        count += 1\n",
    "        step_count += 1\n",
    "        total_rew += rew \n",
    "        \n",
    "        storage.push(step(obs, act_v, next_obs, rew, done))\n",
    "        obs = next_obs\n",
    "        \n",
    "        sample = storage.sample(BATCH)\n",
    "        if sample:\n",
    "            sample = step(*zip(*sample))\n",
    "            \n",
    "            states = torch.FloatTensor(sample.state).cuda()\n",
    "            actions = torch.FloatTensor(sample.action).cuda()\n",
    "            next_states = torch.FloatTensor(sample.next_state).cuda()\n",
    "            rewards = torch.FloatTensor(sample.reward).unsqueeze(-1).cuda()\n",
    "            dones = torch.BoolTensor(sample.done).unsqueeze(-1).cuda()\n",
    "            \n",
    "            # critic learning\n",
    "            critic_optim.zero_grad()\n",
    "            q_pred = critic(states, actions)\n",
    "            \n",
    "            next_action_v = actor_tgt(next_states)\n",
    "            q_next = critic_tgt(next_states, next_action_v)\n",
    "            q_next[dones] = 0\n",
    "            q_target = rewards + GAMMA * q_next\n",
    "            \n",
    "            critic_loss = F.mse_loss(q_pred, q_target.detach())\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "            \n",
    "            # actor learning\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss = -critic(states, actor(states))\n",
    "            actor_loss = actor_loss.mean()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "            \n",
    "            # tgt soft update\n",
    "            for tgt, real  in zip(actor_tgt.parameters(), actor.parameters()):\n",
    "                tgt.data.copy_(TAU*real.data + (1-TAU)*tgt.data)\n",
    "                \n",
    "            for tgt, real  in zip(critic_tgt.parameters(),critic.parameters()):\n",
    "                tgt.data.copy_(TAU*real.data + (1-TAU)*tgt.data)\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "    print(\"epoch %d count %d reward %.5f\"%(epoch, count, total_rew))\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
