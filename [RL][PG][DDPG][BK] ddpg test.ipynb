{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = namedtuple(\"step\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, size):\n",
    "        self.memory = collections.deque(maxlen = size)\n",
    "        \n",
    "    def push(self, data):\n",
    "        self.memory.append(data)\n",
    "        \n",
    "    def prepare(self, env):\n",
    "        pass\n",
    "        \n",
    "    def sample(self, size):\n",
    "        if len(self.memory) >= size:\n",
    "            return random.sample(self.memory, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_n, action_n, hidden = 256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_n, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), action_n),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_n, action_n, hidden =256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_n, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden+action_n, int(hidden/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden/2), 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, act):\n",
    "        temp = self.net(state)\n",
    "        return self.out(torch.cat([temp, act], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "GAME_NAME = \"MountainCar-v0\"\n",
    "\n",
    "env = gym.make(GAME_NAME)\n",
    "obs_n = env.observation_space.shape[0]\n",
    "act_n = env.action_space.n\n",
    "\n",
    "LR = 0.0005\n",
    "TAU = 0.0005\n",
    "GAMMA = 0.99\n",
    "\n",
    "actor = Actor(obs_n, act_n).cuda()\n",
    "actor_optim = optim.Adam(actor.parameters(), lr = LR)\n",
    "actor_tgt = Actor(obs_n, act_n).cuda()\n",
    "actor_tgt.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic = Critic(obs_n, act_n).cuda()\n",
    "critic_optim = optim.Adam(critic.parameters(), lr = LR)\n",
    "critic_tgt = Critic(obs_n, act_n).cuda()\n",
    "critic_tgt.load_state_dict(critic.state_dict())\n",
    "\n",
    "MAX_MEMORY = 20000\n",
    "MEM_INIT = 2000\n",
    "BATCH = 256\n",
    "storage = Replay(MAX_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 count 200\n",
      "epoch 1 count 200\n",
      "epoch 2 count 200\n",
      "epoch 3 count 200\n",
      "epoch 4 count 200\n",
      "epoch 5 count 200\n",
      "epoch 6 count 200\n",
      "epoch 7 count 200\n",
      "epoch 8 count 200\n",
      "epoch 9 count 200\n",
      "epoch 10 count 200\n",
      "epoch 11 count 200\n",
      "epoch 12 count 200\n",
      "epoch 13 count 200\n",
      "epoch 14 count 200\n",
      "epoch 15 count 200\n",
      "epoch 16 count 200\n",
      "epoch 17 count 200\n",
      "epoch 18 count 200\n",
      "epoch 19 count 200\n",
      "epoch 20 count 200\n",
      "epoch 21 count 200\n",
      "epoch 22 count 200\n",
      "epoch 23 count 200\n",
      "epoch 24 count 200\n",
      "epoch 25 count 200\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    obs = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    count = 0\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            act_v = actor(torch.FloatTensor(obs).cuda()).cpu().numpy()\n",
    "            noise = np.random.random(act_n)/(epoch+1)\n",
    "            act_v += noise\n",
    "            act = act_v.argmax().item()\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(act)\n",
    "        rew = next_obs[0]\n",
    "        env.render()\n",
    "        count += 1\n",
    "        \n",
    "        storage.push(step(obs, act_v, next_obs, rew, done))\n",
    "        obs = next_obs\n",
    "        \n",
    "        sample = storage.sample(BATCH)\n",
    "        if sample:\n",
    "            sample = step(*zip(*sample))\n",
    "            \n",
    "            states = torch.FloatTensor(sample.state).cuda()\n",
    "            actions = torch.FloatTensor(sample.action).cuda()\n",
    "            next_states = torch.FloatTensor(sample.next_state).cuda()\n",
    "            rewards = torch.FloatTensor(sample.reward).unsqueeze(-1).cuda()\n",
    "            dones = torch.BoolTensor(sample.done).unsqueeze(-1).cuda()\n",
    "            \n",
    "            # critic learning\n",
    "            critic_optim.zero_grad()\n",
    "            q_pred = critic(states, actions)\n",
    "            \n",
    "            next_action_v = actor_tgt(next_states)\n",
    "            q_next = critic_tgt(next_states, next_action_v)\n",
    "            q_next[dones] = 0\n",
    "            q_target = rewards + GAMMA * q_next\n",
    "            \n",
    "            critic_loss = F.mse_loss(q_pred, q_target.detach())\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "            \n",
    "            # actor learning\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss = -critic(states, actor(states))\n",
    "            actor_loss = actor_loss.mean()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "            \n",
    "            # tgt soft update\n",
    "            for tgt, real  in zip(actor_tgt.parameters(), actor.parameters()):\n",
    "                tgt.data.copy_(TAU*real.data + (1-TAU)*tgt.data)\n",
    "                \n",
    "            for tgt, real  in zip(critic_tgt.parameters(),critic.parameters()):\n",
    "                tgt.data.copy_(TAU*real.data + (1-TAU)*tgt.data)\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "    print(\"epoch %d count %d\"%(epoch, count))\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
