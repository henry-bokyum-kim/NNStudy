{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "import collections\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = \"Pendulum-v0\"\n",
    "env = gym.make(GAME)\n",
    "\n",
    "obs_n = env.observation_space.shape[0]\n",
    "act_n = env.action_space.shape[0]\n",
    "\n",
    "action_scale = env.action_space.high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_MIN = -20\n",
    "STD_MAX = 2\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, hidden = (128, 128)):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_n, hidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(hidden[1], act_n)\n",
    "        self.log_std = nn.Linear(hidden[1], act_n)\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.data.normal_(0, 1e-14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        logit_factor = torch.tanh(self.log_std(x)) + 1\n",
    "        std_logit = STD_MIN + 0.5*(STD_MAX - STD_MIN) * logit_factor\n",
    "        std = torch.exp(std_logit)\n",
    "        \n",
    "        dist = Normal(mu, std)\n",
    "        pi = dist.rsample()\n",
    "        log_pi = dist.log_prob(pi)\n",
    "        \n",
    "        mu = torch.tanh(mu) * action_scale\n",
    "        pi = torch.tanh(pi) * action_scale\n",
    "        \n",
    "        return mu, pi, log_pi\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden = (128, 128)):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_n, hidden[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden[0]+act_n, hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[1], 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs, act):\n",
    "        h = self.net(obs)\n",
    "        x = torch.cat([h, act], 1)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode(env, net=None, show=False):\n",
    "    obs = env.reset()\n",
    "    if show:\n",
    "        env.render()\n",
    "    ep = []\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        if net is None:\n",
    "            act = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                _, pi, log_pi = net(torch.FloatTensor([obs]).to(device))\n",
    "                act = pi.cpu().numpy()[0]\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(act)\n",
    "        if show:\n",
    "            env.render()\n",
    "        count += rew\n",
    "        if done:\n",
    "            print(count, end=' ')\n",
    "            \n",
    "        step = (obs, act, next_obs, rew, done)\n",
    "        ep.append(step)\n",
    "        yield step\n",
    "                \n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "class Memory():\n",
    "    def __init__(self, size):\n",
    "        self.data = collections.deque(maxlen=size)\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.data.append(data)\n",
    "    \n",
    "    def extend(self, data):\n",
    "        self.data.extend(data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def sample(self, size):\n",
    "        assert size <= len(self)\n",
    "        return random.sample(self.data, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1065.4127951244188 -959.334909060996 -1401.4506011561741 -1392.4472609479715 -1305.5754711955979 -896.6362547756713 -886.8930023966681 -1380.3654445152101 -793.4858866253998 -1462.1185644035668 "
     ]
    }
   ],
   "source": [
    "ACT_LR = 3e-5\n",
    "ACT_CLIP = 5e-1\n",
    "actor = Actor().to(device)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr = ACT_LR)\n",
    "\n",
    "CRT_LR = 1e-4\n",
    "CRT_CLIP = 1e0\n",
    "q1 = Critic().to(device)\n",
    "q1_target = Critic().to(device)\n",
    "q1_target.load_state_dict(q1.state_dict())\n",
    "q1_target.eval()\n",
    "\n",
    "q2 = Critic().to(device)\n",
    "q2_target = Critic().to(device)\n",
    "q2_target.load_state_dict(q2.state_dict())\n",
    "q2_target.eval()\n",
    "q_optim = optim.Adam(list(q1.parameters()) + list(q2.parameters()), lr = CRT_LR)\n",
    "\n",
    "EPOCH = 4000\n",
    "BATCH = 64\n",
    "REPEAT = 5\n",
    "GAMMA = 0.99\n",
    "ALPH = 0.2\n",
    "TAU = 1e-2\n",
    "\n",
    "ST_MAX = 20000\n",
    "ST_INIT = 2e3\n",
    "storage = Memory(ST_MAX)\n",
    "while len(storage) < ST_INIT:\n",
    "    ep = get_episode(env)\n",
    "    storage.extend([step for step in ep])\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -1164.2690592025485 22.620750230550765 45.48706240653992\n",
      "1 -1326.3695022035797 67.0290850982666 117.02176022148133\n",
      "2 -1168.1848270693913 104.96734637451172 211.806271900177\n",
      "3 -1281.9446734525613 137.07224115753175 250.96506354904176\n",
      "4 -1411.1252882447227 168.92873377990722 292.6690738353729\n",
      "5 -1618.370359763798 200.71528378295898 406.2027742137909\n",
      "6 -1370.664154445216 227.86358085632324 505.6253550758362\n",
      "7 -1518.383385316265 250.8569178161621 580.5058912925721\n",
      "8 -1569.8677938695412 273.9906424407959 727.8111974906922\n",
      "9 -1050.3983667955392 297.8849736633301 855.0175288162231\n",
      "10 -1192.4537986367898 318.0206702880859 964.5460550689697\n",
      "11 -1425.1604330488606 333.2628228149414 1022.6053234443665\n",
      "12 -1548.2389779067494 343.8177347412109 1219.1489986305237\n",
      "13 -1435.1310521544067 353.96732656860354 1251.4891275787354\n",
      "14 -1370.9793324352747 364.83096267700193 1230.6084582500457\n",
      "15 -1650.1519033706224 376.47714126586914 1260.0177343006135\n",
      "16 -1508.9141825623415 386.38756546020505 1401.7495211734772\n",
      "17 -1586.2292814689401 394.69489862060544 1556.1284021282197\n",
      "18 -1594.3918520390725 402.1944946594238 1552.9923601799012\n",
      "19 -1445.1512994206635 411.163962890625 1681.6501215286255\n",
      "20 -1208.0863552046055 417.6686185913086 1741.2150389175415\n",
      "21 -1654.8977775244934 423.5208243713379 1702.7189174995422\n",
      "22 -1377.3798416532236 428.5040588378906 1746.0351164760589\n",
      "23 -1536.6239631468716 429.38284927368164 1951.9111665534974\n",
      "24 -1004.6496666707917 429.68661550903323 1776.7236601066588\n",
      "25 -1600.4264994809184 432.399808013916 1841.3995280075073\n",
      "26 -1193.1867577621135 434.41494134521486 1878.3411841278075\n",
      "27 -1492.015026593866 440.1691854858398 1619.5245070896149\n",
      "28 -1211.6597620692885 444.50653146362305 1919.6253076896667\n",
      "29 -1567.382545708402 448.19907946777346 1971.9892096633912\n",
      "30 -1491.156397861056 450.7098993225098 2249.1058814163207\n",
      "31 -1574.6755130930442 452.32670434570315 2004.769542427063\n",
      "32 -1500.9188754881757 454.89018023681643 2062.2869819812777\n",
      "33 -1093.171572853009 456.9201301269531 1758.0026265602112\n",
      "34 -1651.0906039512279 458.6246764526367 2048.84151398468\n",
      "35 -1498.847397137822 459.54374700927735 2194.408859960556\n",
      "36 -1197.3476090158217 458.89594741821287 2066.2359140815734\n",
      "37 -1622.9178949344844 461.7645715942383 1834.4293358287812\n",
      "38 -1022.2640013209067 464.4490986022949 1984.8607954235076\n",
      "39 -1651.7598560651354 465.7905151672363 2026.4318617458343\n",
      "40 -1098.0375290467912 466.163953704834 2173.5488789749147\n",
      "41 -1503.9360956245473 465.19543157958987 2378.7139967918397\n",
      "42 -1658.3016172548346 464.5140506591797 2048.6587477111816\n",
      "43 -1523.556966982619 464.4257145385742 2076.8769258403777\n",
      "44 -1493.6131202273953 467.0178395385742 1924.517386587143\n",
      "45 -1635.0213417316047 468.1695965270996 2216.540216625214\n",
      "46 -1653.7817728493817 468.45277938842776 2158.002001876831\n",
      "47 -1049.5291083748627 467.98331869506836 2036.6162569274902\n",
      "48 -1648.051649703024 468.7679098510742 2214.9439248809813\n",
      "49 -1654.6389623341925 470.435755065918 2072.636728866577\n",
      "50 -1605.5283272691747 472.93970718383787 2113.8050672531126\n",
      "51 -1306.635060486819 474.52578121948244 2114.400116527557\n",
      "52 -1657.7608963032162 476.99720907592774 2131.069062099457\n",
      "53 -1644.1829746628264 476.4249189453125 2314.625659563065\n",
      "54 -948.5925078689895 477.80875427246093 2268.262560350418\n",
      "55 -1656.066765545096 478.56459375 1995.2399317817687\n",
      "56 -1380.0430819918254 480.8161643371582 2083.1671507167816\n",
      "57 -1310.748611258505 480.3207576904297 2435.335287530899\n",
      "58 -1650.1922862678641 480.154075592041 2260.102911623001\n",
      "59 -1651.7957911376768 480.9610313415527 2273.8626737651825\n",
      "60 -1440.145289805277 481.51348275756834 2352.87746043396\n",
      "61 -1486.6003302616061 479.9613308105469 2260.253787067413\n",
      "62 -1641.929616617518 480.46051150512693 2241.974979066849\n",
      "63 -1647.7630915103284 481.49573400878904 2209.496266220093\n",
      "64 -1516.9990407424768 483.03695989990234 2100.8759127674102\n",
      "65 -1501.5408218332038 484.3488037109375 2328.168143283844\n",
      "66 -1636.0689774449588 483.9211036682129 2356.226821949005\n",
      "67 -1631.1787311539829 485.78742196655276 2065.7432517147063\n",
      "68 -1497.571523863664 485.900328704834 2476.291526309967\n",
      "69 -1524.2064314878173 486.6275469055176 2351.6650643310545\n",
      "70 -1492.388711654728 486.43700204467774 2307.431368024826\n",
      "71 -1543.4792656885304 486.73414852905273 2200.511035039902\n",
      "72 -1614.8231463624656 487.4804533081055 2380.6542604465485\n",
      "73 -1445.1269054201168 488.19041970825197 2458.038244451523\n",
      "74 -1546.027177161151 487.68815872192386 2219.128979578018\n",
      "75 -1272.6297956149658 488.78247943115235 2218.550975660324\n",
      "76 -1632.4932179849566 488.883720123291 2381.6758684568404\n",
      "77 -1624.5220500505063 489.8885844116211 2377.6209088745118\n",
      "78 -1364.5246339043467 490.1170551147461 2159.8680218105314\n",
      "79 -1634.5298960358969 490.0101887207031 2489.2366650218964\n",
      "80 -1244.373852418768 487.9357986755371 2546.5127970867156\n",
      "81 -1258.4710524139775 487.194309753418 2231.2912074451447\n",
      "82 -1606.7949539323254 487.8996248779297 2152.377983013153\n",
      "83 -1237.9062510695082 487.2912351989746 2321.177922325134\n",
      "84 -1564.2915007488753 487.83690603637694 2355.45719830513\n",
      "85 -1431.12000175949 485.4890621948242 2517.0303593902586\n",
      "86 -1371.1935904882328 484.7003676147461 2242.092886817932\n",
      "87 -1446.9743880601984 482.7246795654297 2585.161447370529\n",
      "88 -1509.1031150262738 482.90032278442385 2259.5972923202517\n",
      "89 -1330.3498943385591 480.04608151245117 2354.1896617565153\n",
      "90 -1547.474964708885 481.24836557006836 2139.4748713226318\n",
      "91 -1251.34431301474 483.54918493652343 2084.757855342865\n",
      "92 -1235.5775783222696 482.5293500671387 2351.234660703659\n",
      "93 -1233.7650010685948 482.6049765319824 2247.6854377002715\n",
      "94 -1450.5163362343073 482.553967376709 2372.1790196199418\n",
      "95 -1324.4320991725824 483.94929278564456 2004.9044551820755\n",
      "96 -1426.1765036783759 484.3954195861816 2250.183395418167\n",
      "97 -1398.7066120911704 484.047347442627 2087.529808883667\n",
      "98 -1419.3622500594172 484.2746369628906 2219.034445623398\n",
      "99 -1495.5775201318022 484.14664349365233 2391.386393357277\n",
      "100 -1312.3130027300735 482.5603300476074 2242.418952327728\n",
      "101 -1391.9520070849173 482.63978692626955 2249.091134051323\n",
      "102 -1482.4537004967058 481.94970901489256 2302.5488481550215\n",
      "103 -1445.2003582831253 481.15460321044924 2180.5354220962527\n",
      "104 -1514.1540787688944 480.71148352050784 2282.510064846039\n",
      "105 -1347.0525839433724 478.9351351318359 2357.6980488729478\n",
      "106 -1136.3552007938804 477.4576618957519 2265.817758621216\n",
      "107 -1584.083201108643 475.0986776428223 2523.2199193153383\n",
      "108 -1501.7689125241855 471.0182575073242 2462.914650905609\n",
      "109 -1121.5906995125945 469.49880279541014 2279.3506042613985\n",
      "110 -1136.929699810062 468.1348158569336 2259.814537477493\n",
      "111 -947.0043056946123 466.53970825195313 2220.0418684301376\n",
      "112 -1059.3152194106956 464.64085220336915 2192.647835044861\n",
      "113 -1143.4388918367904 465.30796936035154 1844.7117403287887\n",
      "114 -1334.1649662060652 465.4499306945801 2099.5051524124146\n",
      "115 -1493.9239432533054 465.82511251831056 2063.9712610292436\n",
      "116 -1119.722930838596 464.64673233032227 2306.7923674259187\n",
      "117 -928.9675594241403 461.03342404174805 2199.717196896553\n",
      "118 -947.1667122685649 459.08904263305664 2045.8719528522493\n",
      "119 -1088.6822036912004 457.4078030395508 2218.022662372589\n",
      "120 -1065.0460592089837 455.0285748901367 1962.999836415291\n",
      "121 -1095.805885823201 455.09211044311525 1939.8993125581742\n",
      "122 -1358.5244769059718 453.4802596435547 2017.9196653146744\n",
      "123 -1236.1448707974632 453.1769902648926 1935.3380695753096\n",
      "124 -1263.5290882408951 451.5754360961914 2088.700327044487\n",
      "125 -1350.9962360855684 448.04898971557617 2371.2320886945727\n",
      "126 -1327.363262651646 445.08043359375 1849.1174286060334\n",
      "127 "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    ep = get_episode(env, actor)\n",
    "    loss = [0,0]\n",
    "    print(epoch, end=' ')\n",
    "    for j, step in enumerate(ep):\n",
    "        storage.append(step)\n",
    "        for i in range(REPEAT):\n",
    "            sample = list(zip(*storage.sample(BATCH)))\n",
    "\n",
    "            obs = torch.FloatTensor(sample[0]).to(device)\n",
    "            probs = torch.FloatTensor(sample[1]).to(device)\n",
    "            next_obs = torch.FloatTensor(sample[2]).to(device)\n",
    "            rew = torch.FloatTensor(sample[3]).to(device).unsqueeze(1)\n",
    "            done = torch.FloatTensor(sample[4]).to(device).unsqueeze(1)\n",
    "\n",
    "            _, cur_pi, cur_log_pi = actor(obs)\n",
    "            _, next_pi, next_log_pi = actor(next_obs)\n",
    "\n",
    "            q1_pred = q1(obs, probs)\n",
    "            q2_pred = q2(obs, probs)\n",
    "\n",
    "            min_q_next = torch.min(q1_target(next_obs, next_pi), q2_target(next_obs, next_pi))\n",
    "            min_q = torch.min(q1(obs, cur_pi), q2(obs, cur_pi))\n",
    "\n",
    "            v_next = (min_q_next - ALPH*next_log_pi)\n",
    "            q_target = rew + GAMMA * (1-done) * v_next\n",
    "            \n",
    "            #\n",
    "\n",
    "            q1_loss = F.mse_loss(q1_pred, q_target.detach())\n",
    "            q2_loss = F.mse_loss(q2_pred, q_target.detach())\n",
    "            q_loss = q1_loss + q2_loss\n",
    "            actor_loss = (ALPH*cur_log_pi - min_q).mean()\n",
    "            \n",
    "            q_optim.zero_grad()\n",
    "            q_loss.backward()\n",
    "            q_optim.step()\n",
    "            \n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "            for l, t in zip(q1.parameters(), q1_target.parameters()):\n",
    "                t.data.copy_(l.data*TAU + t.data*(1-TAU))\n",
    "            for l, t in zip(q2.parameters(), q2_target.parameters()):\n",
    "                t.data.copy_(l.data*TAU + t.data*(1-TAU))\n",
    "            \n",
    "            loss[0] += actor_loss.item()\n",
    "            loss[1] += q_loss.item()\n",
    "    print(loss[0]/(REPEAT*(j+1)), loss[1]/(REPEAT*(j+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor(obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
