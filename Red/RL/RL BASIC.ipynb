{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(GAME)\n",
    "env.reset()\n",
    "act_n = env.action_space.n\n",
    "obs_n = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQNet(nn.Module):\n",
    "    def __init__(self, in_, out_, hidden = 128):\n",
    "        super(BaseQNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCH = 1000\n",
    "SAMPLE = 50\n",
    "bestN = 15\n",
    "net = BaseQNet(obs_n, 1).to(device)\n",
    "\n",
    "opt = optim.Adam(net.parameters(), lr=LR)\n",
    "crt = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20.4 10.444130778312683\n",
      "1 22.62 10.448449313640594\n",
      "2 21.9 10.369653284549713\n",
      "3 23.84 10.430789232254028\n",
      "4 25.54 10.414540410041809\n",
      "5 23.24 10.316456079483032\n",
      "6 24.78 10.223991513252258\n",
      "7 33.08 10.170708298683167\n",
      "8 26.62 10.19842004776001\n",
      "9 27.82 10.126807510852814\n",
      "10 26.56 10.31234472990036\n",
      "11 27.2 10.133429825305939\n",
      "12 30.34 10.063583850860596\n",
      "13 27.04 10.194552421569824\n",
      "14 24.0 10.260365009307861\n",
      "15 33.46 10.029829800128937\n",
      "16 35.26 10.011495232582092\n",
      "17 33.1 9.920941054821014\n",
      "18 37.4 9.948222756385803\n",
      "19 34.2 9.91957277059555\n",
      "20 30.86 10.303567826747894\n",
      "21 34.32 9.983331084251404\n",
      "22 36.22 9.95633590221405\n",
      "23 35.22 9.905102849006653\n",
      "24 38.9 9.750160813331604\n",
      "25 30.84 10.04047167301178\n",
      "26 37.36 9.84970897436142\n",
      "27 40.14 9.848584711551666\n",
      "28 44.1 9.636337757110596\n",
      "29 38.28 9.763657331466675\n",
      "30 51.46 9.692278027534485\n",
      "31 41.88 9.671737849712372\n",
      "32 47.94 9.57748955488205\n",
      "33 53.9 9.513760805130005\n",
      "34 47.38 9.626357913017273\n",
      "35 53.42 9.575250267982483\n",
      "36 56.1 9.464346885681152\n",
      "37 54.46 9.399792194366455\n",
      "38 56.06 9.502787709236145\n",
      "39 57.56 9.456080794334412\n",
      "40 55.48 9.437856376171112\n",
      "41 57.78 9.450978517532349\n",
      "42 66.64 9.464815735816956\n",
      "43 66.34 9.276580214500427\n",
      "44 55.42 9.334395408630371\n",
      "45 63.28 9.371236264705658\n",
      "46 72.34 9.288706600666046\n",
      "47 72.1 9.104011178016663\n",
      "48 66.86 9.253662645816803\n",
      "49 60.3 9.316043972969055\n",
      "50 63.16 9.239468276500702\n",
      "51 64.18 9.059100568294525\n",
      "52 63.3 9.13836133480072\n",
      "53 75.84 8.965255677700043\n",
      "54 78.76 8.95136845111847\n",
      "55 87.18 9.277795732021332\n",
      "56 76.68 9.23510479927063\n",
      "57 77.2 8.894526243209839\n",
      "58 83.26 9.021103501319885\n",
      "59 106.08 8.876221120357513\n",
      "60 101.04 8.814152956008911\n",
      "61 105.92 8.951704561710358\n",
      "62 108.38 8.719542860984802\n",
      "63 98.84 8.841550767421722\n",
      "64 117.96 8.566505134105682\n",
      "65 107.32 8.572511494159698\n",
      "66 108.86 8.688490986824036\n",
      "67 110.32 8.577696561813354\n",
      "68 118.98 8.454863965511322\n",
      "69 117.9 8.551771938800812\n",
      "70 111.28 8.705847263336182\n",
      "71 110.38 8.334901690483093\n",
      "72 105.72 8.20578220486641\n",
      "73 126.92 8.440208494663239\n",
      "74 109.84 8.240827202796936\n",
      "75 102.6 8.30599719285965\n",
      "76 122.18 8.139908611774445\n",
      "77 114.48 7.999474912881851\n",
      "78 123.92 8.13460236787796\n",
      "79 125.08 8.192225933074951\n",
      "80 127.18 8.004921168088913\n",
      "81 128.44 7.992550939321518\n",
      "82 115.68 7.80385747551918\n",
      "83 126.56 7.7683916091918945\n",
      "84 126.08 7.599387675523758\n",
      "85 129.22 7.891490697860718\n",
      "86 135.28 7.791101425886154\n",
      "87 128.8 7.927923321723938\n",
      "88 150.38 7.729132503271103\n",
      "89 137.62 7.575388729572296\n",
      "90 153.78 7.553001970052719\n",
      "91 134.72 7.795922070741653\n",
      "92 160.8 7.670238375663757\n",
      "93 137.8 7.68432143330574\n",
      "94 156.7 7.658474236726761\n",
      "95 169.32 7.578214406967163\n",
      "96 135.56 7.585962474346161\n",
      "97 177.38 7.5758363008499146\n",
      "98 175.2 7.517160922288895\n",
      "99 170.26 7.329338580369949\n",
      "100 177.7 7.205474555492401\n",
      "101 167.48 7.264478236436844\n",
      "102 174.24 7.452352404594421\n",
      "103 187.6 7.284057855606079\n",
      "104 188.58 7.065356492996216\n",
      "105 192.7 7.172406226396561\n",
      "106 203.66 7.045888453722\n",
      "107 181.68 7.17998394370079\n",
      "108 185.4 7.024401277303696\n",
      "109 193.98 7.064438492059708\n",
      "110 189.24 7.136744350194931\n",
      "111 201.66 6.875806897878647\n",
      "112 220.82 6.754168778657913\n",
      "113 209.16 6.818025171756744\n",
      "114 197.2 6.7221550941467285\n",
      "115 215.56 6.715041071176529\n",
      "116 205.32 6.7223305106163025\n",
      "117 217.9 6.804061830043793\n",
      "118 188.88 6.891083300113678\n",
      "119 213.98 6.753568768501282\n",
      "120 210.58 6.729800254106522\n",
      "121 202.86 6.41791170835495\n",
      "122 189.64 6.783705472946167\n",
      "123 200.84 6.646212339401245\n",
      "124 212.2 6.362544357776642\n",
      "125 222.96 6.551580041646957\n",
      "126 211.64 6.338321506977081\n",
      "127 207.86 6.3727594912052155\n",
      "128 226.82 6.375196397304535\n",
      "129 243.58 6.377291977405548\n",
      "130 265.2 6.135225921869278\n",
      "131 252.32 6.528821051120758\n",
      "132 255.78 6.446163475513458\n",
      "133 265.42 6.162278056144714\n",
      "134 271.76 6.09744194149971\n",
      "135 241.08 6.219336152076721\n",
      "136 271.26 6.270085573196411\n",
      "137 290.4 6.295314788818359\n",
      "138 295.22 6.008747011423111\n",
      "139 260.18 6.346282362937927\n",
      "140 277.86 5.866399496793747\n",
      "141 330.82 6.0606328547000885\n",
      "142 268.02 5.948783278465271\n",
      "143 302.02 6.073745280504227\n",
      "144 321.32 5.935132652521133\n",
      "145 316.1 5.752419024705887\n",
      "146 326.46 5.875576019287109\n",
      "147 304.48 5.745938181877136\n",
      "148 326.78 5.943144887685776\n",
      "149 327.72 5.658541411161423\n",
      "150 326.1 5.671232461929321\n",
      "151 305.64 5.775993674993515\n",
      "152 340.64 5.621514409780502\n",
      "153 329.46 5.762329488992691\n",
      "154 356.24 5.854467302560806\n",
      "155 350.54 5.656990051269531\n",
      "156 358.74 5.763923794031143\n",
      "157 363.0 5.740447521209717\n",
      "158 387.54 5.588198661804199\n",
      "159 402.6 5.544362634420395\n",
      "160 403.38 5.7257475554943085\n",
      "161 389.22 5.512349754571915\n",
      "162 427.22 5.383858501911163\n",
      "163 382.4 5.466857522726059\n",
      "164 439.8 5.452494770288467\n",
      "165 435.42 5.207469940185547\n",
      "166 454.44 5.375579863786697\n",
      "167 415.72 5.300201833248138\n",
      "168 433.54 5.399441480636597\n",
      "169 400.92 5.344603449106216\n",
      "170 383.0 5.436957687139511\n",
      "171 418.2 5.2616904973983765\n",
      "172 416.48 5.350341320037842\n",
      "173 422.2 5.258876472711563\n",
      "174 449.94 5.233509331941605\n",
      "175 473.08 5.039026468992233\n",
      "176 452.0 5.222073704004288\n",
      "177 451.36 5.182478189468384\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    #sampling - simulate 50 scenario and take 10 bests.\n",
    "    step_list = []\n",
    "    for sample in range(SAMPLE):\n",
    "        obs = env.reset()\n",
    "        step = []\n",
    "        while True:\n",
    "            if random.random() < np.exp(-epoch/100):\n",
    "                act = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    act_v = net(torch.FloatTensor(obs).to(device))\n",
    "                    act = int(torch.round(act_v).cpu().numpy()[0])\n",
    "            next_obs, rew, done, _ = env.step(act)\n",
    "            step.append((obs, act))\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        step_list.append(step)\n",
    "    sorted_step = sorted(step_list, key=lambda x:len(x), reverse=True)\n",
    "    best_step = sorted_step[:bestN]\n",
    "    \n",
    "    # training\n",
    "    loss_sum = 0\n",
    "    for step in best_step:\n",
    "        step = list(zip(*step))\n",
    "        obs = torch.FloatTensor(step[0]).to(device)\n",
    "        act = torch.FloatTensor(step[1]).unsqueeze(1).to(device)\n",
    "        \n",
    "        pred_act = net(obs)\n",
    "        loss = crt(pred_act, act)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.data.cpu().numpy()\n",
    "    \n",
    "    print(epoch, functools.reduce(lambda prev, cur: (prev + len(cur)), sorted_step, 0)/SAMPLE, loss_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
